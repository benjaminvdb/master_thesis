%%% Preamble
\documentclass[paper=a4, fontsize=11pt]{scrartcl}

\usepackage{mystyle}

%%% Title	
\title{ \vspace{-1in} 	\usefont{OT1}{bch}{b}{n}
		\huge \strut Entity resolution in unstructured data \strut
}
\author{
		\usefont{OT1}{bch}{m}{n} Benjamin van der Burgh\\
		\usefont{OT1}{bch}{m}{n} Leiden University\\
		\usefont{OT1}{bch}{m}{n} Leiden Institute of Advanced Computer Science\\
        \texttt{benjamin.van.der.burgh@liacs.leidenuniv.nl}
}
\date{\today}

%%% Begin document
\begin{document}
\maketitle


% % % % % % % % % % % % % % % % % % % % % %


\section{Introduction}
\label{sec:introduction}

Entity resolution is the process of finding records in one or more datasets that share a common identifier.
Entities in this context commonly refer to people, such as patients and customers, but they can also refer to products, events or any other concept.
Its applications range from duplicate detection to the merging datasets to obtain a single, enriched database. It has a long history of research and a lot of effort has been put into building systems that can perform the task in diverse contexts.
However, many of the techniques developed rely on a database that has a well-defined data model in which specific pieces of information about an entity are mapped to a fixed number of fields.
Whenever unstructured, continuous text is considered, such as commonly found in books and on the web, many of these techniques are not directly applicable and must be modified or perhaps entirely new approaches must be considered.
In this thesis we explore methods to overcome these issues and study ways of extracting information from text to improve entity resolution.

\subsection{Motivation}
\label{sec:motivation}

When studying historical documents, researchers are often confronted with the problem of ambiguous references to people in text.
A document might have been prepared to be of only temporary use or might have been written with a particular set of people in mind, limiting the number of possible people that could be referred to.
However, when the document is studied after many centuries, the exact context is often lost and it will be much harder to identify the persons.
A careful study of other datasets, such as birth registers and censuses, might be needed to figure out who the person under consideration is.
This can be a laborious task and furthermore it can be hard to estimate the quality of the results.

In $2014$ The National Archives (TNA) launched the Traces Through Time project that had to goal of developing a new tool that would enable its users to explore TNA's vast repository of digitized archives in a completely new way.
The system would have to identify occurrences of people in text and decide which of them referred to the same persons.
It would also have to deal with `fuzzy' nature of historical data, including aliases, incomplete information, spelling variations and errors.
This is essentially an entity resolution problem with the added difference that not structured databases, but unstructured documents are to be linked.
This projects shows the need for the development of such as system and serves as an excellent use case.

Another possible application of a system for entity resolution in unstructured text lies in the semantic web.
The semantic web can be seen as a graph in which the nodes represent concepts and are connected by edges representing a type of relation.
By annotating hyperlinks on the web with a type of relation in this way, computers can easily and rapidly parse pages, effectively making the same document suitable for both humans to read and computers to process.
Though the web standards have been extended to support these type of annotations with RDF, it is not used in all pages on the web, often because it is laborious to incorporate semantic information.
Entity resolution in this context could allow for automatic detection of people in text and linking documents together that contain information about the same persons.
This information could also be extracted from text and aggregated into automatically generated summaries, resembling simple Wikipedia pages.
Web searches could be \emph{entity-driven}: besides the ``Images'' and ``News'' button, a search engine could include ``Peson'' enables a user to search for a person.
Results from such a system would aggregate web pages referring the same people together, instead of returning a rather unorganized set of pages.

\subsection{Challenges}
\label{sec:challenges}

Traditionally entity resolution is performed on tables in databases.
Since these usually have a schema, and therefore a structure, we can compute similarities between references by comparing their individual fields.
However, the main focus of this thesis is to study entity resolution in natural language, which lacks such a structure.
Furthermore, the references are not readily accessible and first need to be extracted from text.
We will describe a simple generic rule-based method that can be used to both extract references from text and segment it into a record consisting of fields.
Note that the use of a schema imposes a fixed length on the records, even though it might often happen that one or more attributes are not present.
This becomes more clear with an example.
The following is a section taken from \emph{Fine Roll C 60/33} from the Fine Rolls of King Henry III. \footnote{The Fine Rolls of King Henry III is a dataset consisting of transcribed medieval calendars. This is the same dataset that is used in the experiments described in Chapter ... TODO: add reference.}

\begin{quote}
	Concerning the corn of \ul{Roger of Hyde}. Order to the \ul{sheriff of Oxfordshire} to make the kingâ€™s advantage without delay, by the view of law-worthy men, from all of the corn of \ul{Roger of Hyde, knight}, in Hyde, who is with the Earl Marshal, and to put in gage etc. all those who he will find threshing that corn and intermeddling with the land of the same \ul{Roger} without warrant, to be before the king at his command to answer for it.
\end{quote}

\noindent This paragraph could be segmented in various ways, with one possible outcome given in Table~\ref{tab:segmentation}.
We left out the nouns that refer to people through their jobs, such as king and Earl Marshal, in this example.
The important thing to note is that there are a lot of missing values that have to be dealt with appropriately.

\begin{table}
	\centering
	\input{tables/segmentation.tex}
	\caption{A possible segmentation of the paragraph taken from \emph{Fine Roll C 60/33}.}
	\label{tab:segmentation}
\end{table}

While text in natural language lacks structure, it does not mean that it lacks in information.
Most of the time the text contains a lot of information and a person is referenced within that context.
Therefore it is interesting to look at information that might be contained within the text.
A problem with a rule-based system in this case is that it does not generalize very well.
It is fast and can make good use of regularities in the text, such as the ordering of names, but it is unclear how to apply such rules to extract more general information from the text.
We will therefore study the application of more generic techniques and their impact on the performance of the system.

The core of any record linkage system is the step in which references are classified into matches and non-matches.
In order to do so, a similarity-based approach is often used.
Based on the assumption that references to the same entity are themselves similar, we can start comparing their segmented records.
However, this required a means of comparing records which is not a trivial task.
There are many different approaches, such as counting the number of edit operations required to turn one string into the other, and each come with their own quirks and advantages.
One of the challenges here is to decide select a similarity metric and another is to deal with the missing data such as exemplified in Table~\ref{tab:segmentation}.

Another issue arises when we take the property of \emph{transitivity} into account, i.e., if $a$ and $b$ are a match and $a$ and $c$ are a match, then also $b$ should be a match.
A situation might occur in which this property does not hold, thus we find that there is an inconsistency in the matching status.
Matches can also chain together in which case the ends of the chain are clearly not matches, though intuitively, and by applying the property of transitivity, we could argue that these records should also be matched.
Figure~\ref{fig:transitive_closure} shows an example of such an uncertain situation.
We could argue that the same person was referenced once with his first name and another time with middle name, yet we could also say that there is an inconsistency and perhaps only one pair should be matched.\todo{add numbers as to make the example more clear.}
However, we could also argue that the transitivity property can aid us in the efficient linking of records as it can in fact provide evidence in a similar way that the content of the records do.
Namely, instead of comparing references based on their textual representation we could compare them based on graph similarity.
We will look methods to leverage this property to increase performance while reducing the number of inconsistencies.\todo{this statement is rather bold (it involves some NP-complete problems?), and I don't have a solution, but it's worth trying :).}

\begin{figure}
    \centering
    \input{graphs/transitive_closure.tex}
    \caption{John William Smith has a reasonably high confidence score with both John Smith and William Smith. Inferring from this that also John Smith and William Smith should be matched is however dangerous.}
    \label{fig:transitive_closure}
\end{figure}

The record linkage methods described in this thesis rely on the comparison of references based on their attributes.
Assuming we have no \emph{a priori} knowledge of the match status of each reference pair, the comparison of all possible pairs takes time $\mathcal{O}(n^2)$.
This rapidly becomes too expensive for larger datasets.
Several solutions have been proposed to partially resolve this problem at the expense of decreased sensitivity of the system.
It is not the main focus of this thesis to address this issue, but since it is fairly standard step in the record linkage pipeline, we will shortly discuss it in Section\todo{add reference here}.

\subsubsection{Lack of labeled data}

\subsection{Objectives}
\label{sec:objectives}

\begin{itemize}
	\item To study the application of existing entity resolution techniques in the context of unstructured data.
	\item To study information extraction techniques that can aid the performance of entity resolution.
\end{itemize}

\subsection{Formal problem definition}
\label{sec:formal_problem_definition}

\begin{itemize}
    \item Introduce record linkage as the classification of reference pairs as match or non-match.
    \item The number of unique entities is unknown \emph{a priori} and there is usually a lack of training data.
    \item We can assume that matching pairs have similar comparison vectors.
    \item The natural approach therefore is to use clustering as an unsupervised learning approach.
    \item Classification can be performed by measuring the similarity between objects, essentially clustering objects.
    \item We can then threshold the similarity: pairs above the threshold are classified as matches and below the threshold as non-matches.
\end{itemize}

\subsection{Research questions}
\label{sec:research_questions}

\subsection{Related work}
\label{sec:related_work}

\begin{itemize}
    \item It's probably best to cite literature here that is similar to Traces Through Time, such as ChartEx.
    \item A common probabilistic record linkage approach (Sunter) computes the probability of a pair being a match and equally for being a non-match. The label is assigned according to the highest probability.
    \item This approach makes no use of domain knowledge. What we also use of the probability of observing a certain value (this is related to what literature? cite it).
\end{itemize}

\subsection{Outline}
\label{sec:outline}

% % % % % % % % % % % % % % % % % % % % % %


\section{Record Linker}
\label{sec:record_linker}
In this chapter we will give an overview of the record linkage system that was developed to solve issues that were discussed in \cref{sec:objectives}. It is also the basis of the experiments described in \cref{sec:experiments}.
As we will see, the comparison of references plays a fundamental role in the record linker system described.
However, recalling the mantra ``\emph{garbage-in, garbage out}'' we will also consider the pre- and postprocessing steps involved and treat the resulting pipeline as a unified system.
\todo{give an outline of this chapter by shortly describing the pipeline. Introduce running example.}

\subsection{Data cleaning and segmentation (``record extraction''?)}
\label{sec:segmentation}
Traditional record linkage techniques naturally assume that the input consist of structured records (i.e. tuples).
Out goal however is to perform entity resolution in documents consisting of natural language.
The first step therefore is to extract structured records from the input data.

We have considered using existing tools from the field of Natural Language Processing (NLP), but these posed several issues.
The task is often referred to as \emph{Names-Entity Recognition} (NER) and usually solved with either complicated grammars or statistical (machine learning) approaches.
A problem with existing grammar-based approaches is that they are often too generic and complicated.
Statistical techniques generally rely on the availability of labeled data for training, which is often not the case.
Luckily, the variant of NER we are trying to achieve here is rather simple as it only involves the detection of people in text and not, for example, place names.
Also, person names have a largely well-defined structure and use capitalization, making it a lot easier to detect them.

The method that we settled to using is a simplified version of a grammar based approach and mainly requires a list of first names as a \emph{seed}.
The input is processed as a sequence of tokens, which is generated by simply splitting the text on spaces after removing punctuation; no further processing on the tokens is performed.
After that tokens can be searched for the first names, ignoring tokens that are not recognized as such.
First names are used as anchor points in the grammar by applying rules that try to match tokens based on their relative position to the names.
As an example, let us look at a part from the same part that was cited in \todo{add a number to the quote so we can reference it here. Also, add the grammar that was used so the rules can be referenced to}.

By applying grammar rules in this way surnames can effectively be learned.
These in turn could be used as an anchor point, similar to the first names, which yields a new set of first names that was unknown before.
Repeated application of this procedure can therefore be used to expand a relatively small list of known names into a bigger one in a process called \emph{bootstrapping}.
Caution should be taken, however, to prevent errors made in one iteration of the bootstrapping, since they can propagate to a next iteration.
It is therefore best to manually inspect the learned names after each iteration of the size of the list allows for it.

An advantage of a manually crafted grammar, like the one proposed, is that tokens are classified as part of the parsing, which effectively segments the references into structured records.
These can then be further processed using traditional record linkage techniques.

\subsection{Blocking}
\label{sec:blocking}
As stated in \cref{sec:challenges} for larger datasets it becomes rapidly infeasible to compare all possible pairs of records within a single database, since it grows quadratically.
The total number of possible comparisons is equal to $n\times(n-1)/2$.
Because the similarity between records that we are computing is symmetric it only needs to be computed once.
Since the comparison step is often the most expensive step of the record linkage pipeline, a technique called \emph{blocking} is often deployed.
The goal of blocking is to effienctly partition the set of all possible pairs into several mutually exclusive sets.
Only the records within a set are compared, effectively reducing the number of comparisons that is required.
Of course the goal of record linkage in general is to create such a partitioning, but in blocking this is process is rather coarse-grained and involves computationally cheap operations.

It is easiest to think about the gain of blocking when an example is considered.
Say we have a set of $1000$ records on which we want to perform record linkage.
There are a total number of $450000$ possible pairs that can possibly be compared.
If we were able to partition these pairs into $10$ equally sized \emph{blocks}, only $49500$ comparisons would be needed; a reduction of $89\%$.
Such a reduction is often possible because only a very small fraction of all possible pair as an actual or \emph{true match}, which is often the case.
In the Fine Rolls of King Henry III, that was mentioned in \cref{sec:challenges}, there appear $902$ unique forenames.
It is therefore likely that we will end up with hundreds of unique entities.
Any comparisons that are done between references to these entities, which are of course unknown at the time, would be a waste and should be avoided.

The effectiveness of blocking depends on its ability to partition the references in blocks of roughly equal size, which is often a problem.
Another problem arises when the blocking function is unable to group records together that should have been grouped.
In this case the records will not be compared and are therefore incorrectly classified as non-matches and therefore reduces the system's sensitivity.

Though it is not the main focus of this thesis to study blocking techniques, it is often an important step in the process and we will therefore discuss it shortly.
As we shall see in Section \todo{add reference to experiment chapter}, simple blocking techniques can easily be implemented without only a slight reduction in sensitivity while vastly reducing the number of required comparisons.

\todo{describe a number of blocking techniques.}

\subsection{Field-based comparisons}
\begin{itemize}
    \item The first step in classification is a field-to-field comparison between references.
    \item Forward reference the usage of the outcome, namely that we will threshold the similarity scores, resulting in a binary tuple of the reference likeness.
    \item These tuples are then used, alongside the statistics of the values in the classification of the pairs (see next section).
\end{itemize}

The fundamental principle of the entity resolution techniques described in this thesis are based on the assumption that references to the same entity are relatively similar.
Naturally, if the references are exactly the same we are confident that indeed the references refer to the same real-world entity.
In practice, however, it is not always this obvious, since there many circumstances under which the references differ even though the pair should be classified as a match.
There are are number of reasons that cause these differences, e.g. an error was made during transcription or a name was abbreviated.
To be able to perform entity resolution under such conditions we can make use of approximate string matching techniques.
These work by measuring the proximity of one string to another which gives an intuition about the likeliness of the equivalence of the string.

A distance function $d(\times, \times)$ maps a pair of strings $\sigma_1$ and $\sigma_2$ to a real number $r$, with a greater value of $r$ indicating a smaller similarity between $\sigma_1$ and $\sigma_2$.\cite{cohen03}
Conversely, a similarity function $s(\times, \times$ outputs numbers $r$ with a greater value indicating a larger similarity.
Often these values are normalized such that their value is between $0 \leq r \leq 1$.
Distance and similarity functions for strings must intuitively have a number of properties, which are in general:\cite{christen12}

\begin{itemize}
    \item $s(\sigma_1, \sigma_1)=1$: the similarity is maximal when a string is compared to itself.
    \item $s(\sigma_1, \sigma_2)=0$: a 'completely different' string results in the minimum similarity value of $0$.
    \item $0 < s(\sigma_1, \sigma_2) < 1$: a value between the two extremes indicates that the strings are 'somewhat similar'.
\end{itemize}

With similarity values between $0$ and $1$ it is easy to convert to distance values by subtracting them from $1$, and vice versa.
It depends on the calculation of the proximity measure whether it makes more sense to talk about one or the other.
For the record linker it does not make a huge difference as the interpretation of the numbers is simply inversed.
We will see that a threshold can then be set for each component, mapping the scores to binary values, or used as-is in the classification of references which will be further discussed in \todo{add section reference here}.

The remaining section describes a number of distance metrics that were used in our record linkage system to perform pairwise comparisons between the fields of a pair of records, which we assume to be of the string data type.
We denote with $\sigma$ some string constructed from individual literals taken from an alphabet $\Sigma$ and use the superscript notation, e.g. $\sigma^{i}$, to refer to its constituent parts.
The notation $\vert \times \vert$ is used to refer to the length of a string.
Note that we are now considering the field strings in isolation and not the records as a whole, hence a \emph{match} in this context means that the fields are ''similar enough``.
Further processing the results of this phase is further discussed in ...

\subsubsection{Edit and Levenshtein distance}
\label{sec:edit_distance}
As stated, misspellings and typographical errors are a common source of errors and often turn up as a single character being replaced or perhaps an additional being added.
A natural way to compute a distance between two strings is therefore the smallest number of modifications that is needed for one of the strings to be turned into the other.
This is the underlying principle of the class of edit distances.

In its simplest form, three operations are considered: substitutions, insertions and deletions, each of them associated with a cost of $1$.
This variant of the edit distance is often called the \emph{Levenshtein distance} \todo{add reference to Levenshtein's article}.
The edit distance can efficiently be computed in time $\mathcal{O}(\vert \sigma_1 \times \sigma_2 \vert)$ using a dynamic programming approach.
It breaks up the computation into smaller subproblems, namely the computation of the edit distance between all possible prefixes of one string and all possible prefixes of the second.
The algorithm for computing edit distances is given in Algorithm~\ref{alg:edit_distance}.
First thing to note is that the first row of the matrix keeps track of the cost of deleting literals from $B$ while the first column keeps track of deleting literals from $A$.
These can be filled in without requiring any knowledge of the strings themselves, except for their length.
A cell $D[i,j]$ in the matrix corresponds to the number of edits required to convert the first $i$ characters of the string $\sigma_1$ (shown in the first column of a matrix) into the string comprised of the first $j$ characters of string $\sigma_2$ (shown in top row of a matrix).\cite{christen12}

If we look at the example as depicted in Figure~\ref{fig:edit_distance}, we can see how the algorithm computes the Levenshtein distance (i.e. the edit distance with equal cost of all edit operations) between Owen (British) and Owain (Welsh).
The computation starts in the top left corner with an initial score of $0$.
Aligning the literal 'O' of Owain with the empty string results in a mismatch and induces a cost of $1$, therefore $D[0, 1]=1$.
A vertical move down requires similar reasoning and also results in a cost of $1$.
Aligning both starting letters results in a match and the diagonal move introduces no cost, therefore $D[1, 1]=0$.
This process is repeated until all cells of the matrix have been computed.
The edit distance can be found in the lower right corner and is $2$ in this case.
Shown in bold is a construction path that shows one of the possible alignments associated with the computed distance.
It is constructed by backtracking from the lower right corner to the top left each of the steps that led to the minimum value, i.e. the lowest value of the vertical, horizontal and diagonal moves must be selected.
From the example it can be seen that a choice sometimes occurs, since multiple alignments can exist for a certain edit distance.

\begin{algorithm}
    \input{algorithms/edit_distance.tex}
    \caption{Computes the edit distance between two strings $\sigma_1$ and $\sigma_2$}
    \label{alg:edit_distance}
\end{algorithm}

\begin{figure}
    \centering
    \begin{minipage}{.65\textwidth}
        \centering
        \input{tables/edit_distance_matrix.tex}
    \end{minipage}%
    \begin{minipage}{.35\textwidth}
        \centering
        \begin{tabular}{c||c|c|c|c|c}
            $\sigma_1$ & O & w & a & i & n \\\hline
            $\sigma_2$ & O & w & e & \_ & n
        \end{tabular}
    \end{minipage}
    \caption{The matrix in the left shows the computation of the edit distance between Owen and Owain. The construction path, presented in bold, can be seen as substituting the 'a' for an 'e' and removing the 'i' from Owain as to obtain Owen, as depicted on the right.}
    \label{fig:edit_distance}
\end{figure}

\subsubsection{Q-gram string matching}
\label{sec:qgram}
Another class of approximate string similarity functions are based on \emph{Q-grams}, sometimes called \emph{n-grams}.
A q-gram is a sub-sequence of $q$ characters.
Often selected values for $q$ are $q=2$ (called bigrams or digrams) and $q=3$ (called trigrams).
They can be obtained from an input string by moving a sliding window of width $q$ over the string and counting occurrences of each q-gram encountered, effectively computing a multiset.
The general approach of q-gram string matching is to measure similarity of the obtained multisets using the number of q-grams that they have in common.
Many of ways exist for comparing multisets, but three are most often used.

\begin{align}
    \text{Overlap coefficient: }sim_overlap(\sigma_1, \sigma_2) &= \frac{c_{common}}{min(c_1, c2)} \\
    \text{Jaccard coefficient: }sim_jaccard(\sigma_1, \sigma_2) &= \frac{c_{common}}{c_1 + c_2 - c_{common}} \\
    \text{Dice coefficient: }sim_dice(\sigma_1, \sigma_2) &= \frac{2 \times c_{common}}{c_1 + c_2}
\end{align}

\noindent A comprehensive overview of multiset distances can be found in \cite{kosters08}.

An advantage of q-gram based functions is that they are efficiently computable.
Computation involves the extraction of q-grams from both strings, which can be computed in time $\mathcal{O}(\vert \sigma_1 \vert + \vert + \sigma_2 \vert$, followed by the computation of the intersection, computed in $\mathcal{O}$.
Compared to the time complexity of $\mathcal{O}(\vert \sigma_1 \vert \times \vert \sigma_2 \vert)$ of the edit distance this an improvement.
A disadvantage is that, due to the nature of a set, information is lost about the ordering of the q-grams in the string.
For example, the strings 'abc' and 'bca' both include the bigram 'bc', but in a different place.
While a method based on edit distance would penalize accordingly, a q-gram based method will simply disregard this fact.

\subsubsection{Jaro and Jaro-Winkler similarity}
\label{sec:jaro}
The family of Jaro similarity functions combine some of the advantages of both the edit distance and q-gram similarity.
It counts the number of characters $c$ that are in common within a window that is of size $\lfloor \frac{\text{max}(\SizeOf{\sigma_1}, \SizeOf{\sigma_2})}{2}\rfloor-1$.
Moreover, the Jaro similarity function also accounts for the number of transpositions $t$, i.e., adjacent characters that are swapped in the two strings.
Put together, the metric is defined as follows:

\begin{equation}
    s_{jaro}(\sigma_1, \sigma_2) =
    \begin{dcases}
        \frac{1}{3} \left( \frac{c}{\vert \sigma_1 \vert} + \frac{c}{\vert \sigma_2 \vert} + \frac{c-t}{c} \right) & \text{if } m = 0 \\
        0 & \text{otherwise}
    \end{dcases}
\end{equation}

In the context of record linkage it makes sense to spend time on improving approximate similarity functions on a specific use case, namely for names.
Names regularly appear abbreviated in text, but this does not change the prefix.
The Jaro-Winkler similarity function takes this into account by increasing the similarity value based on the number of agreeing characters at the beginning of the two strings.
It is computed as follows:

\begin{equation}
    s_winkler(\sigma_1, \sigma_2)=s_jaro(\sigma_1, \sigma_2) + (1-s_jaro(\sigma_1, \sigma_2)\frac{p}{10}
\end{equation}

\noindent with $p\in\{0,\dots,4\}$ being the length of the longest common prefix of at most $4$.

\begin{figure}
    \centering
    \begin{tabular}{?c | ^c | ^c | ^c | ^c | ^c | ^c | ^c |}
        \rowstyle{\itshape}
        Index      & 0 & 1 & 2 & 3 & 4 & 5 & 6 \\\hline
        $\sigma_1$ & \textbf{j} & \textbf{o} & n & e & s &   &   \\		
        Matches    & 0 & 1 & 4 & - & 3 & - & - \\		
        $\sigma_2$ & \textbf{j} & \textbf{o} & h & s & n & o & n \\\hline
    \end{tabular}
    \caption{This table shows the matching of individual characters between string 'jones' and 'johsnon'. The longest sequence of matching starting characters is $2$ and is printed in bold face.}
    \label{fig:jaro_winkler}
\end{figure}

\cref{fig:jaro_winkler} shows how individual characters in the strings 'jones' and 'johsnon' are matched.
We can see that four characters can be matches and there is one transposition, since one character must be swapped to turn the list of indexes into a sorted array.
The Jaro simmilarity can be computed as follows.

\begin{equation*}
    s_{jaro}('jones', 'johsnon') = \frac{1}{3} \left(\frac{4}{5} + \frac{4}{7} + \frac{3}{4} \right) = 0.70714
\end{equation*}

\noindent The number of matching starting character is $2$, which is used by the Jaro-Winker distance to increase the similarity as follows:

\begin{align*}
    s_{winkler}('jones', 'johsnon') &= s_{jaro}('jones', 'johsnon') + (1-s_{jaro}('jones', 'johsnon')\frac{p}{10} \\
    &= 0.70714 + (1 - 0.70714)\times\frac{2}{10} \\
    &= 0.76571
\end{align*}

\subsubsection{Soundex and editex phonetic similarity}
Another class of approximate string similarities is the \emph{Soundex} phonetic similarity.
It aims to take into account the phonetics, i.e., the auditory properties of the strings, with emphasis on the \emph{homophones}.
These are words that are pronounced in the same way, but are written differently, such as road/rode/rowed or seas/sees/seize.
Ambiguity caused by homophones in conversation are often resolved because of context.
This does not apply to names, however, which may cause a name to be misspelled.
An illustrative example of a special case of homophones can be found in datasets SC8 and C53.\footnote{Add references to http://discovery.nationalarchives.gov.uk/details/r/C3613 and http://discovery.nationalarchives.gov.uk/details/r/C13526}
In C53 there is a mentioned of ``Walter de Gloucestre'' and in SC8 a mention of ``Walter de Gloucester''.
If these are indeed the same people -- this is unfortunately impossible to know for certain -- then this misspelling was probably caused by a cross-language homophone.
Such differences in spelling can be dealt with by devising a phonetic string comparison function, such as soundex.

A soundex encoding of a string consists of the first character of the string, followed by three digits that encode the remaining consonants.
Consonants that are often pronounced similarly are mapped to the same character.
In total, procedure is as follows:

\begin{enumerate}
    \item The starting character of the string is used as the first character of the encoding.
    \item Remove all occurrences of `a', `e', `i', `o', `'u', `y', `h' and `w'.
    \item Substitute consonants after the first character with digits as follows:\\
            \begin{align*}
                b, f, p, v &\rightarrow 1 \\
                c, g, j, k, q, s, x, z &\rightarrow 2 \\
                d, t &\rightarrow 3 \\
                l &\rightarrow 4 \\
                m, n &\rightarrow 5 \\
                r &\rightarrow 6
            \end{align*}
\end{enumerate}

Additionally, the following rules are applied\footnote{reference http://www.archives.gov/research/census/soundex.html}:

\begin{itemize}
    \item \textbf{Names with double letters}: whenever the surname has any double letters, they should be treated as one letter.
    \item \textbf{Names with letters side-by-side that have the same soundex code number}: if the surname has different letters side-by-side that have the same number in the soundex coding guide, they should be treated as one letter.
    \item \textbf{Names with prefixes}: if a surname has a prefix, such as `van', `con', `de', `di', `la', or `le', code both with and without the prefix because the surname might be listed under either code.
    \item \textbf{Consonant separators}: if a vowel separates two consonants that have the same soundex code, the consonant to the right of the vowel is coded.
\end{itemize}

A drawback of the soundex encoding is that the rules are specifically geared towards a certain language, English, in this case.
To capture the phonetics of another language a different mapping must be developed and tested.
Variants for the Indian language are available\footnote{http://airccse.org/journal/ijcsea/papers/4314ijcsea03.pdf}.

Soundex encodings can be used in approximate string matching by checking the encodings of the input strings for equality, resulting in a binary number.
A different approach is to apply the idea of treating phonetically similar values equivalently in way comparable to the edit distance.
The similarity function \emph{editex} modifies the cost function of the edit distance slightly.

\begin{equation}
    c_{editex}(a_i, a_j) =
    \begin{dcases}
        0 & \text{if } a_i = a_j \\
        1 & \text{if } a_i \neq a_j \wedge a_i \equiv_s a_j \\
        2 & \text{otherwise}
    \end{dcases}
\end{equation}

\noindent where $\equiv_s$ is the relation that two characters belong to the same equivalence class in soundex, i.e., they map to the same character, such as `b' and `f'.
This approach has proven to be quite successful for matching names\cite{zobel96}.
Since it is computed analogous to the edit distance it is also computed in time $\mathcal{O}(\SizeOf{\sigma_1} \times \SizeOf{\sigma_2})$.

\subsection{Reference pair classification}

\subsubsection{Comparison of binary feature vectors}
\begin{itemize}
    \item Classification is generally done on feature vectors as resulting from the field-based comparisons.
    \item Various methods have been proposed, such as unsupervised, supervised, active-learning and incremental.
    \item Labeled data is often lacking so unsupervised methods are most widely applicable.
\end{itemize}

\subsubsection{Probabilistic record linkage}
\begin{itemize}
    \item In probabilistic record linkage the values in the records are weighted according to an external source of statistics.
    \item Give an example which naturally shows how statistics / informative value aids the linkage.
    \item Define equation that is used in the computation.
    \item Discuss the relation to joint entropy?
\end{itemize}

\subsection{Leveraging transitivity (?)}
\begin{itemize}
    \item Transitivity can lead to inconsistencies in the inference of the record linker.
    \item However, evidence can be obtained by applying transitivity, since nodes that share a relatively high number of edges, i.e., are graphically similar, might refer to the same entity.
\end{itemize}

\subsection{Data fusion (?)}
\begin{itemize}
    \item Though it is not the primary goal of this thesis, it might be interesting to consider an example of a possible next step in the process.
    \item Discuss how an entity-driven web search might aggregate information about a person.
\end{itemize}


% % % % % % % % % % % % % % % % % % % % % %


\section{Feature Extraction}

%\subsection{Rule-based reference extraction}
% I am discussing this as the first step of the pipeline now.

%\subsection{NLP-driven reference extraction (?)}
% I will probably not discuss this topic in detail, though I might use it to compare the described rule-based system to.

\subsection{k-Maximally informative itemsets}

\subsection{Co-occurrence (?)}


% % % % % % % % % % % % % % % % % % % % % %


\section{Experimental Evaluation}
\label{sec:experiments}

\subsection{Datasets}

\subsubsection{Fine Rolls}

\begin{itemize}
	\item Overview of the dataset (structure, size, etc.)
	\item First parsing: conversion to JSON.
	\item Second parsing: entity extraction from text.
\end{itemize}


\subsubsection{Wikipedia}
	
\begin{itemize}
	\item Overview of used data dump (structure, size, etc.)
	\item Parsing of the data.
	\item ...
\end{itemize}

\subsection{Experimental evaluation of distance metrics}

\subsection{Experimental evaluation of MIKIs}

\begin{table}
    \begin{minipage}{.5\textwidth}
        \small
        \centering
    	\sisetup{round-mode=places}
    	\begin{tabular}{l S[round-precision=4]}
    		\toprule
    		{Item} & {Entropy}\\
    		\midrule
    		mark & 0.9994486557970268\\
    		king & 1.8147018826248247\\
    		befor & 2.6235330642969554\\
    		son & 3.2605532800247521\\
    		sheriff & 3.8850819263595207\\
    		taken & 4.3911933509382104\\
    		writ & 4.8598517328911237\\
    		aforesaid & 5.3099654282367048\\
    		wife & 5.7269049944269224\\
    		land & 6.1245725613443947\\
    		exchequ & 6.4903439188828891\\
    		half & 6.8427723677764876\\
    		thi & 7.1546285665244032\\
    		yorkshir & 7.4341218502511719\\
    		render & 7.6827923893317687\\
    		norfolk & 7.919293824195619\\
    		lincolnshir & 8.138467999949345\\
    		justic & 8.341568005640335\\
    		fine & 8.5119982948892652\\
    		somerset & 8.6624825596208694\\
    		\bottomrule
    	\end{tabular}
    	\label{t:miki_finerolls}
    \end{minipage}
    \begin{minipage}{.5\textwidth}
        \small
        \centering
    	\sisetup{round-mode=places}
    	\begin{tabular}{l S[round-precision=4]}
    		\toprule
    		{Item} & {Entropy}\\
    		\midrule
    		allow & 0.99999711460799467\\
    		univers & 1.9885280695560985\\
    		german & 2.9554587534421453\\
    		histori & 3.885064693145309\\
    		presid & 4.80108986272643\\
    		near & 5.6856158645882138\\
    		given & 6.5352458324908422\\
    		left & 7.2978334845386019\\
    		project & 7.9730729769060034\\
    		possibl & 8.5001227573930844\\
    		refer & 8.8938672662529648\\
    		servic & 9.1868378998084061\\
    		physic & 9.3978310762587931\\
    		written & 9.5430385108901277\\
    		work & 9.6547452728720131\\
    		employ & 9.738368181814776\\
    		thi & 9.7988366838674423\\
    		chemic & 9.8509902971425589\\
    		includ & 9.8925000721468823\\
    		minist & 9.9206648691832591\\
    		\bottomrule
    	\end{tabular}
    	\label{t:miki_wikipedia}
    \end{minipage}
    \caption{The tables above show the 20-miki as computed on the Fine Rolls of king Henry III (left) and the Wikipedia subset seeded from the page about Albert Einstein (right).}
\end{table}

\begin{figure}
    \input{plots/miki_finerolls.tex}%
    ~
    \input{plots/miki_wikipedia.tex}
    \caption{Plots showing the convergence behavior for the Fine Rolls of King Henry III dataset (left) and the Wikipedia dataset seeded with Albert Einstein (right).}\label{fig:1}
\end{figure}

\subsection{Experimental evaluation of Record Linkage}


% % % % % % % % % % % % % % % % % % % % % %


\section{Conclusions and future work}


% % % % % % % % % % % % % % % % % % % % % %


\section{Bibliography}

\begingroup
\renewcommand{\section}[2]{}%
%\renewcommand{\chapter}[2]{}% for other classes
\bibliographystyle{plain}
\bibliography{mybib}
\endgroup


% % % % % % % % % % % % % % % % % % % % % %


\end{document}