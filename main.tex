%%% Preamble
\documentclass[paper=a4, fontsize=11pt]{scrartcl}

\usepackage{mystyle}

%%% Title	
\title{ \vspace{-1in} 	\usefont{OT1}{bch}{b}{n}
		\huge \strut Entity resolution in unstructured data \strut
}
\author{
		\usefont{OT1}{bch}{m}{n} Benjamin van der Burgh\\
		\usefont{OT1}{bch}{m}{n} Leiden University\\
		\usefont{OT1}{bch}{m}{n} Leiden Institute of Advanced Computer Science\\
        \texttt{benjamin.van.der.burgh@liacs.leidenuniv.nl}
}
\date{\today}

%%% Begin document
\begin{document}
\maketitle


% % % % % % % % % % % % % % % % % % % % % %


\section{Introduction}

\subsection{Record Linkage in unstructured data}
\begin{itemize}
	\item Record Linkage is the process of disambiguating references.
	\item The thesis focuses on unstructured text as can be found in archives and the web.
\end{itemize}

Entity resolution is the process of finding records in one or more datasets that share a common identifier.
Entities in this context commonly refer to people, such as patients and customers, but they can also refer to products, events or any other concept.
Its applications range from duplicate detection to merging datasets to obtain a single, enriched database.
	
Entity resolution, also known as record linkage or data matching, has a long history of research and a lot of effort has been put into building systems that can perform the task in diverse contexts.
However, many of the techniques developed rely on a database with a well-defined data model in which specific pieces of information about an entity are mapped to a fixed number of fields.
Whenever unstructured, continuous text is considered, such as commonly found in books and on the web, many of these techniques are not directly applicable and must be modified or perhaps entirely new approaches must be considered.

\subsection{Motivation}
\begin{itemize}
	\item Explain the problem The National Archives have.
	\item Discuss other possible applications such as entity-driven web search.
	\item Disambiguation and deduplication can help to build the semantic web by linking together different data sources.
\end{itemize}

When studying historical documents, researchers are often confronted with the problem of ambiguous references to people in text.
A document might have been prepared to be of only temporary use or might have been written with a particular set of people in mind, limiting the number of possible people that could be referred to.
However, when the document is studied after many centuries, the exact context is often lost and it will be much harder to identify the persons.
A careful study of other datasets, such as birth registers and censuses, might be needed to figure out who the person under consideration is.
This can be a laborious task and furthermore it can be hard to estimate the quality of the results.

In $2014$ The National Archives (TNA) launched the Traces Through Time project that had to goal of developing a new tool that would enable its users to explore TNA's vast repository of digitized archives in a completely new way.
The system would have to identify occurrences of people in text and decide which of them referred to the same persons.
It would also have to deal with `fuzzy' nature of historical data, including aliases, incomplete information, spelling variations and errors.
This is essentially an entity resolution problem with the added difference that not structured databases, but unstructured documents are to be linked.
This projects shows the need for the development of such as system and serves as an excellent use case.

\subsection{Challenges}
Traditionally entity resolution is performed on tables in databases.
Since these usually have a schema, and therefore a structure, we can compute similarities between references by comparing their individual fields.
However, the main focus of this thesis is to study entity resolution in natural language, which lacks such a structure.
Furthermore, the references are not readily accessible and first need to be extracted from text.
We will describe a simple generic rule-based method that can be used to both extract references from text and segment it into a record consisting of fields.
Note that the use of a schema imposes a fixed length on the records, even though it might often happen that one or more attributes are not present.
This becomes more clear with an example.
The following is a section taken from \emph{Fine Roll C 60/33} from the Fine Rolls of King Henry III. \footnote{The Fine Rolls of King Henry III is a dataset consisting of transcribed medieval calendars. This is the same dataset that is used in the experiments described in Chapter ... TODO: add reference.}

\begin{quote}
	Concerning the corn of \ul{Roger of Hyde}. Order to the \ul{sheriff of Oxfordshire} to make the kingâ€™s advantage without delay, by the view of law-worthy men, from all of the corn of \ul{Roger of Hyde, knight}, in Hyde, who is with the Earl Marshal, and to put in gage etc. all those who he will find threshing that corn and intermeddling with the land of the same \ul{Roger} without warrant, to be before the king at his command to answer for it.
\end{quote}

\noindent This paragraph could be segmented in various ways, with one possible outcome given in Table~\ref{tab:segmentation}.
We left out the nouns that refer to people through their jobs, such as king and Earl Marshal, in this example.
The important thing to note is that there are a lot of missing values that have to be dealt with appropriately.

\begin{table}
	\centering
	\input{tables/segmentation.tex}
	\caption{A possible segmentation of the paragraph taken from \emph{Fine Roll C 60/33}.}
	\label{tab:segmentation}
\end{table}

While text in natural language lacks structure, it does not mean that it lacks in information.
Most of the time the text contains a lot of information and a person is referenced within that context.
Therefore it is interesting to look at information that might be contained within the text.
A problem with a rule-based system in this case is that it does not generalize very well.
It is fast and can make good use of regularities in the text, such as the ordering of names, but it is unclear how to apply such rules to extract more general information from the text.
We will therefore study the application of more generic techniques and their impact on the performance of the system.

The core of any record linkage system is the step in which references are classified into matches and non-matches.
In order to do so, a similarity-based approach is often used.
Based on the assumption that references to the same entity are themselves similar, we can start comparing their segmented records.
However, this required a means of comparing records which is not a trivial task.
There are many different approaches, such as counting the number of edit operations required to turn one string into the other, and each come with their own quirks and advantages.
One of the challenges here is to decide select a similarity metric and another is to deal with the missing data such as exemplified in Table~\ref{tab:segmentation}.

Another issue arises when we take the property of \emph{transitivity} into account, i.e., if $a$ and $b$ are a match and $a$ and $c$ are a match, then also $b$ should be a match.
We could make use of this property to find more matches or, conversely, we could exclude certain matches if there is an inconsistency.
Figure~\ref{fig:transitive_closure} shows an example of such an uncertain situation.
We could argue that the same person was referenced once with his first name and another time with middle name, yet we could also say that there is an inconsistency and perhaps only one pair should be matched.\todo{add numbers as to make the example more clear.}

\begin{figure}
    \centering
    \input{graphs/transitive_closure.tex}
    \caption{John William Smith has a reasonably high confidence score with both John Smith and William Smith. Inferring from this that also John Smith and William Smith should be matched is however dangerous.}
    \label{fig:transitive_closure}
\end{figure}

\subsubsection{Inconsistencies under the transitivity assumption}
\begin{itemize}
    \item We can treat the output of the pair classification step as a labeled set and use it to deduce more labels using the assumption of transitivity, i.e., if $(\alpha, \beta)$ is a match and $(\alpha, \gamma)$ is a match, then also $(\beta, \gamma)$ is a match (\emph{closing triangles}).
    \item Give graphical example of transitive closure.
    \item Minimizing the number of inconsistencies is an NP-complete problem \cite{bansal04}.
    \item This can lead to inconsistencies where $s_c(\beta, \gamma)$ is very low and therefore likely not a match.
\end{itemize}

\subsubsection{Scalability (?)}
\begin{itemize}
	\item A naive entity resolution algorithm will compare all possible pairs of references, which grow quadratically with the number of references.
	\item In order to be able to use the system on a large scale, some pruning can be performed on the candidate pairs.
	\item Before doing a computationally expensive full comparison we group references together that have something in common, e.g. a forename, in a process that is usually called blocking.
\end{itemize}

\subsubsection{Lack of labeled data}

\subsection{Objectives}
\begin{itemize}
	\item To study the application of existing entity resolution techniques in the context of unstructured data.
	\item To study information extraction techniques that can aid the performance of entity resolution.
\end{itemize}

\subsection{Formal problem definition}
\begin{itemize}
    \item Introduce record linkage as the classification of reference pairs as match or non-match.
    \item The number of unique entities is unknown \emph{a priori} and there is usually a lack of training data.
    \item We can assume that matching pairs have similar comparison vectors.
    \item The natural approach therefore is to use clustering as an unsupervised learning approach.
    \item Classification can be performed by measuring the similarity between objects, essentially clustering objects.
    \item We can then threshold the similarity: pairs above the threshold are classified as matches and below the threshold as non-matches.
\end{itemize}

\subsection{Research questions}

\subsection{Related work}
\begin{itemize}
    \item A common probabilistic record linkage approach (Sunter) computes the probability of a pair being a match and equally for being a non-match. The label is assigned according to the highest probability.
    \item This approach makes no use of domain knowledge. What we also use of the probability of observing a certain value.
\end{itemize}

\subsection{Outline}


% % % % % % % % % % % % % % % % % % % % % %


\section{Record Linker}

\subsection{Record linkage pipeline}
\begin{itemize}
    \item This section serves as an introduction to the rest of the chapter.
    \item Provide an overview of the record linkage pipeline with its various stages.
    \item Introduce a running example that shows some of the decisions that must be made.
\end{itemize}

\subsection{Data cleaning and segmentation}
\begin{itemize}
    \item The source data consists of natural language.
    \item Natural language is in an ambiguous format.
    \item In order to use traditional record linkage techniques, we need to extract the interesting parts of text and segment it into a fixed tabular format.
    \item One way of writing about entity resolution is to first talk about ''references`` and about ''records`` after segmentation.
\end{itemize}

\subsection{Blocking}
\begin{itemize}
    \item A naive comparison of all pairs has a time complexity that is quadratic in the number of references.
    \item For large scale record linkage it is infeasible to do an expensive comparison.
    \item One solution is to apply blocking, which partitions references that are similar to some degree together.
    \item Instead of doing an expensive comparison, the references are partitioned into blocks and references within these blocks are compared.
\end{itemize}

\subsection{Field-based comparisons}
\begin{itemize}
    \item The first step in classification is a field-to-field comparison between references.
    \item Forward reference the usage of the outcome, namely that we will threshold the similarity scores, resulting in a binary tuple of the reference likeness.
    \item These tuples are then used, alongside the statistics of the values in the classification of the pairs (see next section).
\end{itemize}

The fundamental principle of the entity resolution techniques described in this thesis are based on the assumption that references to the same entity are relatively similar.
Naturally, if the references are exactly the same we are confident that indeed the references refer to the same real-world entity.
In practice, however, it is not always this obvious, since there many circumstances under which the references differ even though the pair should be classified as a match.
There are are number of reasons that cause these differences, e.g. an error was made during transcription or a name was abbreviated.
To be able to perform entity resolution under such conditions we can make use of approximate string matching.
This is usually achieved by computing a similarity score, usually in $[0 \dots 1]$, between the individual fields that make up the records.
A threshold can then be set for each component, mapping the scores to binary values, or used as-is in the classification of references which will be further discussed in \todo{add section reference here}.

The remaining section describes a number of distance metrics that were used in our record linkage system to perform pairwise comparisons between the fields of a pair of records, which we assume to be of the string data type.
We denote with $\sigma$ some string constructed from individual literals taken from an alphabet $\Sigma$ and use the superscript notation, e.g. $\sigma^{i}$, to refer to its constituent parts.
The notation $\vert \times \vert$ is used to refer to the length of a string.
Note that we are now considering the field strings in isolation and not the records as a whole, hence a \emph{match} in this context means that the fields are ''similar enough``.
Further processing the results of this phase is further discussed in ...

\subsubsection{Edit and Levenshtein distance}
As stated, misspellings and typographical errors are a common source of errors and often turn up as a single character being replaced or perhaps an additional being added.
A natural way to compute a distance between two strings is therefore the smallest number of modifications that is needed for one of the strings to be turned into the other.
This is the underlying principle of the class of edit distances.

In its simplest form, three operations are considered: substitutions, insertions and deletions, each of them associated with a cost of $1$.
This variant of the edit distance is often called the \emph{Levenshtein distance} \todo{add reference to Levenshtein's article}.
The edit distance can efficiently be computed in time $\mathcal{O}(\vert \sigma_1 \times \sigma_2 \vert)$ using a dynamic programming approach.
It breaks up the computation into smaller subproblems, namely the computation of the edit distance between all possible prefixes of one string and all possible prefixes of the second.
The algorithm for computing edit distances is given in Algorithm~\ref{alg:edit_distance}.
First thing to note is that the first row of the matrix keeps track of the cost of deleting literals from $B$ while the first column keeps track of deleting literals from $A$.
These can be filled in without requiring any knowledge of the strings themselves, except for their length.
A cell $D[i,j]$ in the matrix corresponds to the number of edits required to convert the first $i$ characters of the string $\sigma_1$ (shown in the first column of a matrix) into the string comprised of the first $j$ characters of string $\sigma_2$ (shown in top row of a matrix) \todo{Literary copy-paste from Peter Christen's "Data Matching". Edit and/or add reference.}

If we look at the example as depicted in Figure~\ref{fig:edit_distance}, we can see how the algorithm computes the Levenshtein distance (i.e. the edit distance with equal cost of all edit operations) between Owen (British) and Owain (Welsh).
The computation starts in the top left corner with an initial score of $0$.
Aligning the literal 'O' of Owain with the empty string results in a mismatch and induces a cost of $1$, therefore $D[0, 1]=1$.
A vertical move down requires similar reasoning and also results in a cost of $1$.
Aligning both starting letters results in a match and the diagonal move introduces no cost, therefore $D[1, 1]=0$.
This process is repeated until all cells of the matrix have been computed.
The edit distance can be found in the lower right corner and is $2$ in this case.
Shown in bold is a construction path that shows one of the possible alignments associated with the computed distance.
It is constructed by backtracking from the lower right corner to the top left each of the steps that led to the minimum value, i.e. the lowest value of the vertical, horizontal and diagonal moves must be selected.
From the example it can be seen that a choice sometimes occurs, since multiple alignments can exist for a certain edit distance.

\begin{algorithm}
    \input{algorithms/edit_distance.tex}
    \caption{Computes the edit distance between two strings $\sigma_1$ and $\sigma_2$}
    \label{alg:edit_distance}
\end{algorithm}

\begin{figure}
    \centering
    \begin{minipage}{.65\textwidth}
        \centering
        \input{tables/edit_distance_matrix.tex}
    \end{minipage}%
    \begin{minipage}{.35\textwidth}
        \centering
        \begin{tabular}{c||c|c|c|c|c}
            $\sigma_1$ & O & w & a & i & n \\\hline
            $\sigma_2$ & O & w & e & \_ & n
        \end{tabular}
    \end{minipage}
    \caption{The matrix in the left shows the computation of the edit distance between Owen and Owain. The construction path, presented in bold, can be seen as substituting the 'a' for an 'e' and removing the 'i' from Owain as to obtain Owen, as depicted on the right.}
    \label{fig:edit_distance}
\end{figure}

\subsubsection{Jaro distance}

\subsection{Reference pair classification}

\subsubsection{Comparison of binary feature vectors}
\begin{itemize}
    \item Classification is generally done on feature vectors as resulting from the field-based comparisons.
    \item Various methods have been proposed, such as unsupervised, supervised, active-learning and incremental.
    \item Labeled data is often lacking so unsupervised methods are most widely applicable.
\end{itemize}

\subsubsection{Probabilistic record linkage}
\begin{itemize}
    \item In probabilistic record linkage the values in the records are weighted according to an external source of statistics.
    \item Give an example which naturally shows how statistics / informative value aids the linkage.
    \item Define equation that is used in the computation.
    \item Discuss the relation to joint entropy?
\end{itemize}

\subsection{Data fusion (?)}
\begin{itemize}
    \item Though it is not the primary goal of this thesis, it might be interesting to consider an example of a possible next step in the process.
    \item Discuss how an entity-driven web search might aggregate information about a person.
\end{itemize}


% % % % % % % % % % % % % % % % % % % % % %


\section{Feature Extraction}

\subsection{Rule-based reference extraction}

\subsection{NLP-driven reference extraction (?)}

\subsection{k-Maximally informative itemsets}

\subsection{Co-occurrence (?)}


% % % % % % % % % % % % % % % % % % % % % %


\section{Experimental Evaluation}

\subsection{Datasets}

\subsubsection{Fine Rolls}

\begin{itemize}
	\item Overview of the dataset (structure, size, etc.)
	\item First parsing: conversion to JSON.
	\item Second parsing: entity extraction from text.
\end{itemize}


\subsubsection{Wikipedia}
	
\begin{itemize}
	\item Overview of used data dump (structure, size, etc.)
	\item Parsing of the data.
	\item ...
\end{itemize}

\subsection{Experimental evaluation of distance metrics}

\subsection{Experimental evaluation of MIKIs}

\begin{table}
    \begin{minipage}{.5\textwidth}
        \small
        \centering
    	\sisetup{round-mode=places}
    	\begin{tabular}{l S[round-precision=4]}
    		\toprule
    		{Item} & {Entropy}\\
    		\midrule
    		mark & 0.9994486557970268\\
    		king & 1.8147018826248247\\
    		befor & 2.6235330642969554\\
    		son & 3.2605532800247521\\
    		sheriff & 3.8850819263595207\\
    		taken & 4.3911933509382104\\
    		writ & 4.8598517328911237\\
    		aforesaid & 5.3099654282367048\\
    		wife & 5.7269049944269224\\
    		land & 6.1245725613443947\\
    		exchequ & 6.4903439188828891\\
    		half & 6.8427723677764876\\
    		thi & 7.1546285665244032\\
    		yorkshir & 7.4341218502511719\\
    		render & 7.6827923893317687\\
    		norfolk & 7.919293824195619\\
    		lincolnshir & 8.138467999949345\\
    		justic & 8.341568005640335\\
    		fine & 8.5119982948892652\\
    		somerset & 8.6624825596208694\\
    		\bottomrule
    	\end{tabular}
    	\label{t:miki_finerolls}
    \end{minipage}
    \begin{minipage}{.5\textwidth}
        \small
        \centering
    	\sisetup{round-mode=places}
    	\begin{tabular}{l S[round-precision=4]}
    		\toprule
    		{Item} & {Entropy}\\
    		\midrule
    		allow & 0.99999711460799467\\
    		univers & 1.9885280695560985\\
    		german & 2.9554587534421453\\
    		histori & 3.885064693145309\\
    		presid & 4.80108986272643\\
    		near & 5.6856158645882138\\
    		given & 6.5352458324908422\\
    		left & 7.2978334845386019\\
    		project & 7.9730729769060034\\
    		possibl & 8.5001227573930844\\
    		refer & 8.8938672662529648\\
    		servic & 9.1868378998084061\\
    		physic & 9.3978310762587931\\
    		written & 9.5430385108901277\\
    		work & 9.6547452728720131\\
    		employ & 9.738368181814776\\
    		thi & 9.7988366838674423\\
    		chemic & 9.8509902971425589\\
    		includ & 9.8925000721468823\\
    		minist & 9.9206648691832591\\
    		\bottomrule
    	\end{tabular}
    	\label{t:miki_wikipedia}
    \end{minipage}
    \caption{The tables above show the 20-miki as computed on the Fine Rolls of king Henry III (left) and the Wikipedia subset seeded from the page about Albert Einstein (right).}
\end{table}

\begin{figure}
    \input{plots/miki_finerolls.tex}%
    ~
    \input{plots/miki_wikipedia.tex}
    \caption{Plots showing the convergence behavior for the Fine Rolls of King Henry III dataset (left) and the Wikipedia dataset seeded with Albert Einstein (right).}\label{fig:1}
\end{figure}

\subsection{Experimental evaluation of Record Linkage}


% % % % % % % % % % % % % % % % % % % % % %


\section{Conclusions and future work}


% % % % % % % % % % % % % % % % % % % % % %


\section{Bibliography}

\bibliographystyle{plain}
\bibliography{mybib}


% % % % % % % % % % % % % % % % % % % % % %


\end{document}