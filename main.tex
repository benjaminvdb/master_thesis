%%% Preamble
\documentclass[paper=a4, fontsize=11pt]{scrartcl}

\usepackage{mystyle}

%%% Title	
\title{ \vspace{-1in} 	\usefont{OT1}{bch}{b}{n}
		\huge \strut Entity resolution in unstructured data \strut
}
\author{
		\usefont{OT1}{bch}{m}{n} Benjamin van der Burgh\\
		\usefont{OT1}{bch}{m}{n} Leiden University\\
		\usefont{OT1}{bch}{m}{n} Leiden Institute of Advanced Computer Science\\
        \texttt{benjamin.van.der.burgh@liacs.leidenuniv.nl}
}
\date{\today}

%%% Begin document
\begin{document}
\maketitle


% % % % % % % % % % % % % % % % % % % % % %


\section{Introduction}

\subsection{Record Linkage in unstructured data}
\begin{itemize}
	\item Record Linkage is the process of disambiguating references.
	\item The thesis focuses on unstructured text as can be found in archives and the web.
\end{itemize}

Entity resolution is the process of finding records in one or more datasets that share a common identifier. Entities in this context commonly refer to people, such as patients and customers, but they can also refer to products, events or any other concept. Its applications range from duplicate detection to merging datasets to obtain a single, enriched database.
	
Entity resolution, also known as record linkage or data matching, has a long history of research and a lot of effort has been put into building systems that can perform the task in diverse contexts. However, many of the techniques developed rely on a database with a well-defined data model in which specific pieces of information about an entity are mapped to a fixed number of fields. Whenever unstructured, continuous text is considered, such as commonly found in books and on the web, many of these techniques are not directly applicable and must be modified or perhaps entirely new approaches must be considered.

\subsection{Motivation}
\begin{itemize}
	\item Explain the problem The National Archives have.
	\item Discuss other possible applications such as entity-driven web search.
	\item Disambiguation and deduplication can help to build the semantic web by linking together different data sources.
\end{itemize}

When studying historical documents, researchers are often confronted with the problem of ambiguous references to people in text. A document might have been prepared to be of only temporary use or might have been written with a particular set of people in mind, limiting the number of possible people that could be referred to. However, when the document is studied after many centuries, the exact context is often lost and it will be much harder to identify the persons. A careful study of other datasets, such as birth registers and censuses, might be needed to figure out who the person under consideration is. This can be a laborious task and furthermore it can be hard to estimate the quality of the results.

In $2014$ The National Archives (TNA) launched the Traces Through Time project that had to goal of developing a new tool that would enable its users to explore TNA's vast repository of digitized archives in a completely new way. The system would have to identify occurrences of people in text and decide which of them referred to the same persons. It would also have to deal with `fuzzy' nature of historical data, including aliases, incomplete information, spelling variations and errors. This is essentially an entity resolution problem with the added difference that not structured databases, but unstructured documents are to be linked. This projects shows the need for the development of such as system and serves as an excellent use case.

\subsection{Challenges}
\subsubsection{Unstructured data}
\begin{itemize}
	\item Traditionally entity resolution is performed on tables in databases. Since these usually have a schema, and therefore a structure, we can compute similarities between references by comparing their individual fields.
	\item In order to be able to apply traditional techniques on unstructured data we first need to segment the references into rows that comply to a predefined schema.
	\item Due to the lack of structure in the texts the mapping may result in (a lot of) missing values which need to be dealt with correctly.
\end{itemize}

\subsubsection{Information extraction}
\begin{itemize}
	\item Most importantly references need to be extracted from the text.
	\item A rule-based system can be used that works by utilizing a predefined set of forenames as a seed and retrieves the title, surnames, etc. from the surrounding text.
	\item More advanced Natural Language Processing (NLP) techniques can be governed to potentially improve recall.
	\item In this thesis we also study the inclusion of contextual information in the record linkage process.
\end{itemize}

\subsubsection{Pair classification}
\begin{itemize}
	\item The core of entity resolution is the classification of candidate pairs.
	\item In a probabilistic record linker a confidence score is computed for each candidate pair.
	\item It is based on the informational value of the individual fields of the occurrences and measures our confidence in case of a match.
\end{itemize}

\subsubsection{Scalability (?)}
\begin{itemize}
	\item A naive entity resolution algorithm will compare all possible pairs of references, which grow quadratically with the number of references.
	\item In order to be able to use the system on a large scale, some pruning can be performed on the candidate pairs.
	\item Before doing a computationally expensive full comparison we group references together that have something in common, e.g. a forename, in a process that is usually called blocking.
\end{itemize}

\subsection{Objectives}
\begin{itemize}
	\item To study the application of existing entity resolution techniques in the context of unstructured data.
	\item To study information extraction techniques that can aid the performance of entity resolution.
\end{itemize}

\subsection{Formal problem definition}

\subsection{Research questions}

\subsection{Related work}

\subsection{Outline}


% % % % % % % % % % % % % % % % % % % % % %


\section{Record Linker}

\subsection{Record linkage pipeline}
\begin{itemize}
    \item This section serves as an introduction to the rest of the chapter.
    \item Provide an overview of the record linkage pipeline with its various stages.
    \item Introduce a running example that shows some of the decisions that must be made.
\end{itemize}

\subsection{Data cleaning and segmentation}
\begin{itemize}
    \item The source data consists of natural language.
    \item Natural language is in an ambiguous format.
\end{itemize}

\subsection{Indexing / partitioning}
\begin{itemize}
    \item A naive comparison of all pairs has a time complexity that is quadratic in the number of references.
    \item For large scale record linkage it is infeasible to do an expensive comparison.
    \item One solution is to apply blocking, which partitions references that are similar to some degree together.
    \item Instead of doing an expensive comparison, the references are partitioned into blocks and references within these blocks are compared.
\end{itemize}

\subsection{Reference pair comparisons}

\subsubsection{Field-based comparisons}
\begin{itemize}
    \item The first step in classification is a field-to-field comparison between references.
    \item Forward reference the usage of the outcome, namely that we will threshold the similarity scores, resulting in a binary tuple of the reference likeness.
    \item These tuples are then used, alongside the statistics of the values in the classification of the pairs (see next section).
\end{itemize}

\subsubsection{Absolute distance}
\subsubsection{Edit distance}
\subsubsection{Jaro distance}

\subsection{Classification of pairs}

\subsubsection{Comparison of binary feature vectors}
\begin{itemize}
    \item Classification is generally done on feature vectors as resulting from the field-based comparisons.
    \item Various methods have been proposed, such as unsupervised, supervised, active-learning and incremental.
    \item Labeled data is often lacking so unsupervised methods are most widely applicable.
\end{itemize}

\subsubsection{Probabilistic record linkage}
\begin{itemize}
    \item In probabilistic record linkage the values in the records are weighted according to an external source of statistics.
    \item Give an example which naturally shows how statistics / informative value aids the linkage.
    \item Define equation that is used in the computation.
    \item Discuss the relation to joint entropy?
\end{itemize}

\subsection{Data fusion (?)}
\begin{itemize}
    \item Though it is not the primary goal of this thesis, it might be interesting to consider an example of a possible next step in the process.
    \item Discuss how an entity-driven web search might aggregate information about a person.
\end{itemize}


% % % % % % % % % % % % % % % % % % % % % %


\section{Feature Extraction}

\subsection{Rule-based reference extraction}

\subsection{NLP-driven reference extraction (?)}

\subsection{k-Maximally informative itemsets}


% % % % % % % % % % % % % % % % % % % % % %


\section{Experimental Evaluation}

\subsection{Datasets}

\subsubsection{Fine Rolls}

\begin{itemize}
	\item Overview of the dataset (structure, size, etc.)
	\item First parsing: conversion to JSON.
	\item Second parsing: entity extraction from text.
\end{itemize}


\subsubsection{Wikipedia}
	
\begin{itemize}
	\item Overview of used data dump (structure, size, etc.)
	\item Parsing of the data.
	\item ...
\end{itemize}

\subsection{Experimental evaluation of distance metrics}

\subsection{Experimental evaluation of MIKIs}

\begin{table}
    \begin{minipage}{.5\textwidth}
        \small
        \centering
    	\sisetup{round-mode=places}
    	\begin{tabular}{l S[round-precision=4]}
    		\toprule
    		{Item} & {Entropy}\\
    		\midrule
    		mark & 0.9994486557970268\\
    		king & 1.8147018826248247\\
    		befor & 2.6235330642969554\\
    		son & 3.2605532800247521\\
    		sheriff & 3.8850819263595207\\
    		taken & 4.3911933509382104\\
    		writ & 4.8598517328911237\\
    		aforesaid & 5.3099654282367048\\
    		wife & 5.7269049944269224\\
    		land & 6.1245725613443947\\
    		exchequ & 6.4903439188828891\\
    		half & 6.8427723677764876\\
    		thi & 7.1546285665244032\\
    		yorkshir & 7.4341218502511719\\
    		render & 7.6827923893317687\\
    		norfolk & 7.919293824195619\\
    		lincolnshir & 8.138467999949345\\
    		justic & 8.341568005640335\\
    		fine & 8.5119982948892652\\
    		somerset & 8.6624825596208694\\
    		\bottomrule
    	\end{tabular}
    	\label{t:miki_finerolls}
    \end{minipage}
    \begin{minipage}{.5\textwidth}
        \small
        \centering
    	\sisetup{round-mode=places}
    	\begin{tabular}{l S[round-precision=4]}
    		\toprule
    		{Item} & {Entropy}\\
    		\midrule
    		allow & 0.99999711460799467\\
    		univers & 1.9885280695560985\\
    		german & 2.9554587534421453\\
    		histori & 3.885064693145309\\
    		presid & 4.80108986272643\\
    		near & 5.6856158645882138\\
    		given & 6.5352458324908422\\
    		left & 7.2978334845386019\\
    		project & 7.9730729769060034\\
    		possibl & 8.5001227573930844\\
    		refer & 8.8938672662529648\\
    		servic & 9.1868378998084061\\
    		physic & 9.3978310762587931\\
    		written & 9.5430385108901277\\
    		work & 9.6547452728720131\\
    		employ & 9.738368181814776\\
    		thi & 9.7988366838674423\\
    		chemic & 9.8509902971425589\\
    		includ & 9.8925000721468823\\
    		minist & 9.9206648691832591\\
    		\bottomrule
    	\end{tabular}
    	\label{t:miki_wikipedia}
    \end{minipage}
    \caption{The tables above show the 20-miki as computed on the Fine Rolls of king Henry III (left) and the Wikipedia subset seeded from the page about Albert Einstein (right).}
\end{table}

\begin{figure}
    \input{plots/miki_finerolls.tex}%
    ~
    \input{plots/miki_wikipedia.tex}
    \caption{Plots showing the convergence behavior for the Fine Rolls of King Henry III dataset (left) and the Wikipedia dataset seeded with Albert Einstein (right).}\label{fig:1}
\end{figure}

\subsection{Experimental evaluation of Record Linkage}


% % % % % % % % % % % % % % % % % % % % % %


\section{Conclusions and future work}


% % % % % % % % % % % % % % % % % % % % % %


\section{Bibliography}


% % % % % % % % % % % % % % % % % % % % % %


\end{document}