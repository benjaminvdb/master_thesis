%- We can assume that matching pairs have similar comparison vectors.
%- The natural approach therefore is to use clustering as an unsupervised learning approach.
%- Classification can be performed by measuring the similarity between objects, essentially clustering %objects.
%- We can then threshold the similarity: pairs above the threshold are classified as matches and below the threshold as non-matches.

\section{Record Linker}
\label{sec:record_linker}

In this chapter we will give an overview of the record linkage system that was developed to solve issues that were discussed in \cref{sec:problem_definition}. It is also the basis of the experiments described in \cref{sec:experiments}.
As we will see, the comparison of references plays a fundamental role in the record linker system described.
However, recalling the mantra ``\emph{garbage-in, garbage out}'' we will also consider the pre- and postprocessing steps involved and treat the resulting pipeline as a unified system.

\textbf{Give an outline of this chapter by shortly describing the pipeline. Introduce running example.}


% % % % % % % % % % % % % % % % % % % % % %


\subsection{Data cleaning and segmentation (``record extraction''?)}
\label{sec:segmentation}

Traditional record linkage techniques naturally assume that the input consist of structured records (i.e. tuples).
Our goal however is to perform entity resolution in documents consisting of natural language.
The first step therefore is to extract structured records from the input data.

We have considered using existing tools from the field of Natural Language Processing (NLP), but these posed several issues.
The task is often referred to as \emph{Names-Entity Recognition} (NER) and usually solved with either complicated grammars or statistical (machine learning) approaches.
A problem with existing grammar-based approaches is that they are often too generic and complicated.
Statistical techniques generally rely on the availability of labeled data for training, which is often not the case.
Luckily, the variant of NER we are trying to achieve here is rather simple as it only involves the detection of people in text and not, for example, place names.
Also, person names have a largely well-defined structure and use capitalization, making it a lot easier to detect them.

The method that we settled on using is a simplified version of a grammar based approach and mainly requires a list of first names as a \emph{seed}.
The input is processed as a sequence of tokens, which is generated simply by splitting the text on spaces after removing punctuation; no further processing on the tokens is performed.
After that, tokens can be searched for the first names, ignoring tokens that are not recognized as such.
First names are used as anchor points in the grammar by applying rules that try to match tokens based on their relative position to the names.
As an example, let us look at a part from the same part that was cited in \todo{add a number to the quote so we can reference it here. Also, add the grammar that was used so the rules can be referenced to}.

By applying grammar rules in this way surnames can effectively be learned.
These in turn could be used as an anchor point, similar to the first names, which yields a new set of first names that were unknown before.
Repeated application of this procedure can consequently be used to expand a relatively small list of known names into a bigger one in a process called \emph{bootstrapping}.
Caution should be taken, however, to prevent errors made in one iteration of the bootstrapping, since they can propagate to a next iteration.
It is therefore best to manually inspect the learned names after each iteration of the size of the list allows for it.

An advantage of a manually crafted grammar, like the one proposed, is that tokens are classified as part of the parsing, which effectively segments the references into structured records.
These can then be further processed using traditional record linkage techniques.


% % % % % % % % % % % % % % % % % % % % % %


\subsection{Blocking}
\label{sec:blocking}

As stated in \cref{sec:challenges}, for larger datasets it becomes rapidly infeasible to compare all possible pairs of records within a single database, since it grows quadratically.
The total number of possible record links, and thus comparisons in a naive setting, with a dataset is equal to $n\times(n-1)/2$.
Because the similarity between records that we are computing is symmetric it only needs to be computed once.
Since the comparison step is often the most expensive step of the record linkage pipeline, a technique called \emph{blocking} is often deployed.
The goal of blocking is to efficiently partition the set of all possible pairs into several mutually exclusive sets.
Only the records within a set are compared, effectively reducing the number of comparisons that is required.
Note that even though the goal of record linkage itself is to create such a partitioning, in blocking this is process is rather coarse-grained, and involves computationally cheap operations.

It is easiest to think about the gain of blocking when an example is considered.
Say we have a set of $1000$ records on which we want to perform record linkage.
There are a total number of $450000$ possible pairs that can possibly be compared.
If we were able to partition these pairs into $10$ equally sized \emph{blocks}, only $49500$ comparisons would be needed; a reduction of $89\%$.
Such a reduction is often possible because only a very small fraction of all possible pair as an actual or \emph{true match}, which is often the case.
In the Fine Rolls of King Henry III, that was mentioned in \cref{sec:challenges}, there appear $902$ unique forenames.
It is therefore likely that we will end up with hundreds of unique entities.
Any comparisons that are done between references to these entities, which are of course unknown at the time, would be a waste and should be avoided.

The effectiveness of blocking depends on its ability to partition the references in blocks of roughly equal size, which is often a problem.
Another problem arises when the blocking function is unable to group records together that should have been grouped.
In this case the records will not be compared and are therefore incorrectly classified as non-matches and therefore reduces the system's sensitivity.

Though it is not the main focus of this thesis to study blocking techniques, it is often an important step in the process and we will therefore discuss it shortly.
As we shall see in Section \todo{add reference to experiment chapter}, simple blocking techniques can easily be implemented without only a slight reduction in sensitivity while vastly reducing the number of required comparisons.

\todo{describe a number of blocking techniques.}


% % % % % % % % % % % % % % % % % % % % % %


\subsection{Field-based comparisons}
\label{sec:field_comparisons}

\begin{itemize}
    \item The first step in classification is a field-to-field comparison between references.
    \item Forward reference the usage of the outcome, namely that we will threshold the similarity scores, resulting in a binary tuple of the reference likeness.
    \item These tuples are then used, alongside the statistics of the values in the classification of the pairs (see next section).
\end{itemize}

The fundamental principle of the entity resolution techniques described in this thesis are based on the assumption that references to the same entity are relatively similar.
Naturally, if the references are exactly the same we are confident that indeed the references refer to the same real-world entity.
In practice, however, it is not always this obvious, since there many circumstances under which the references differ even though the pair should be classified as a match.
There are are number of reasons that cause these differences, e.g. an error was made during transcription or a name was abbreviated.
To be able to perform entity resolution under such conditions we can make use of approximate string matching techniques.
These work by measuring the proximity of one string to another which gives an intuition about the likeliness of the equivalence of the string.

A distance function $d(\times, \times)$ maps a pair of strings $\sigma_1$ and $\sigma_2$ to a real number $r$, with a greater value of $r$ indicating a smaller similarity between $\sigma_1$ and $\sigma_2$.\cite{cohen03}
Conversely, a similarity function $s(\times, \times$ outputs numbers $r$ with a greater value indicating a larger similarity.
Often these values are normalized such that their value is between $0 \leq r \leq 1$.
Distance and similarity functions for strings must intuitively have a number of properties, which are in general:\cite{christen12}

\begin{itemize}
    \item $s(\sigma_1, \sigma_1)=1$: the similarity is maximal when a string is compared to itself.
    \item $s(\sigma_1, \sigma_2)=0$: a 'completely different' string results in the minimum similarity value of $0$.
    \item $0 < s(\sigma_1, \sigma_2) < 1$: a value between the two extremes indicates that the strings are 'somewhat similar'.
\end{itemize}

With similarity values between $0$ and $1$ it is easy to convert to distance values by subtracting them from $1$, and vice versa.
It depends on the calculation of the proximity measure whether it makes more sense to talk about one or the other.
For the record linker it does not make a huge difference as the interpretation of the numbers is simply inversed.
We will see that a threshold can then be set for each component, mapping the scores to binary values, or used as-is in the classification of references which will be further discussed in \todo{add section reference here}.

The remaining section describes a number of distance metrics that were used in our record linkage system to perform pairwise comparisons between the fields of a pair of records, which we assume to be of the string data type.
We denote with $\sigma$ some string constructed from individual literals taken from an alphabet $\Sigma$ and use the superscript notation, e.g. $\sigma^{i}$, to refer to its constituent parts.
The notation $\vert \times \vert$ is used to refer to the length of a string.
Note that we are now considering the field strings in isolation and not the records as a whole, hence a \emph{match} in this context means that the fields are ''similar enough``.
Further processing the results of this phase is further discussed in ...


% % % % % % % % % % % % % % % % % % % % % %


\subsubsection{Edit and Levenshtein distance}
\label{sec:edit_distance}

As stated, misspellings and typographical errors are a common source of errors and often turn up as a single character being replaced or perhaps an additional being added.
A natural way to compute a distance between two strings is therefore the smallest number of modifications that is needed for one of the strings to be turned into the other.
This is the underlying principle of the class of edit distances.

In its simplest form, three operations are considered: substitutions, insertions and deletions, each of them associated with a cost of $1$.
This variant of the edit distance is often called the \emph{Levenshtein distance} \todo{add reference to Levenshtein's article}.
The edit distance can efficiently be computed in time $\mathcal{O}(\vert \sigma_1 \times \sigma_2 \vert)$ using a dynamic programming approach.
It breaks up the computation into smaller subproblems, namely the computation of the edit distance between all possible prefixes of one string and all possible prefixes of the second.
The algorithm for computing edit distances is given in Algorithm~\ref{alg:edit_distance}.
First thing to note is that the first row of the matrix keeps track of the cost of deleting literals from $B$ while the first column keeps track of deleting literals from $A$.
These can be filled in without requiring any knowledge of the strings themselves, except for their length.
A cell $D[i,j]$ in the matrix corresponds to the number of edits required to convert the first $i$ characters of the string $\sigma_1$ (shown in the first column of a matrix) into the string comprised of the first $j$ characters of string $\sigma_2$ (shown in top row of a matrix).\cite{christen12}

If we look at the example as depicted in Figure~\ref{fig:edit_distance}, we can see how the algorithm computes the Levenshtein distance (i.e. the edit distance with equal cost of all edit operations) between Owen (British) and Owain (Welsh).
The computation starts in the top left corner with an initial score of $0$.
Aligning the literal 'O' of Owain with the empty string results in a mismatch and induces a cost of $1$, therefore $D[0, 1]=1$.
A vertical move down requires similar reasoning and also results in a cost of $1$.
Aligning both starting letters results in a match and the diagonal move introduces no cost, therefore $D[1, 1]=0$.
This process is repeated until all cells of the matrix have been computed.
The edit distance can be found in the lower right corner and is $2$ in this case.
Shown in bold is a construction path that shows one of the possible alignments associated with the computed distance.
It is constructed by backtracking from the lower right corner to the top left each of the steps that led to the minimum value, i.e. the lowest value of the vertical, horizontal and diagonal moves must be selected.
From the example it can be seen that a choice sometimes occurs, since multiple alignments can exist for a certain edit distance.

\begin{algorithm}
    \input{algorithms/edit_distance.tex}
    \caption{Computes the edit distance between two strings $\sigma_1$ and $\sigma_2$}
    \label{alg:edit_distance}
\end{algorithm}

\begin{figure}
    \centering
    \begin{minipage}{.65\textwidth}
        \centering
        \input{tables/edit_distance_matrix.tex}
    \end{minipage}%
    \begin{minipage}{.35\textwidth}
        \centering
        \begin{tabular}{c||c|c|c|c|c}
            $\sigma_1$ & O & w & a & i & n \\\hline
            $\sigma_2$ & O & w & e & \_ & n
        \end{tabular}
    \end{minipage}
    \caption{The matrix in the left shows the computation of the edit distance between Owen and Owain. The construction path, presented in bold, can be seen as substituting the 'a' for an 'e' and removing the 'i' from Owain as to obtain Owen, as depicted on the right.}
    \label{fig:edit_distance}
\end{figure}


% % % % % % % % % % % % % % % % % % % % % %

\subsubsection{Q-gram string matching}
\label{sec:qgram}

Another class of approximate string similarity functions are based on \emph{Q-grams}, sometimes called \emph{n-grams}.
A q-gram is a sub-sequence of $q$ characters.
Often selected values for $q$ are $q=2$ (called bigrams or digrams) and $q=3$ (called trigrams).
They can be obtained from an input string by moving a sliding window of width $q$ over the string and counting occurrences of each q-gram encountered, effectively computing a multiset.
The general approach of q-gram string matching is to measure similarity of the obtained multisets using the number of q-grams that they have in common.
Many of ways exist for comparing multisets, but three are most often used.

\begin{align}
    \text{Overlap coefficient: }sim_overlap(\sigma_1, \sigma_2) &= \frac{c_{common}}{min(c_1, c2)} \\
    \text{Jaccard coefficient: }sim_jaccard(\sigma_1, \sigma_2) &= \frac{c_{common}}{c_1 + c_2 - c_{common}} \\
    \text{Dice coefficient: }sim_dice(\sigma_1, \sigma_2) &= \frac{2 \times c_{common}}{c_1 + c_2}
\end{align}

\noindent A comprehensive overview of multiset distances can be found in \cite{kosters08}.

An advantage of q-gram based functions is that they are efficiently computable.
Computation involves the extraction of q-grams from both strings, which can be computed in time $\mathcal{O}(\vert \sigma_1 \vert + \vert + \sigma_2 \vert$, followed by the computation of the intersection, computed in $\mathcal{O}$.
Compared to the time complexity of $\mathcal{O}(\vert \sigma_1 \vert \times \vert \sigma_2 \vert)$ of the edit distance this an improvement.
A disadvantage is that, due to the nature of a set, information is lost about the ordering of the q-grams in the string.
For example, the strings 'abc' and 'bca' both include the bigram 'bc', but in a different place.
While a method based on edit distance would penalize accordingly, a q-gram based method will simply disregard this fact.


% % % % % % % % % % % % % % % % % % % % % %


\subsubsection{Jaro and Jaro-Winkler similarity}
\label{sec:jaro}

The family of Jaro similarity functions combine some of the advantages of both the edit distance and q-gram similarity.
It counts the number of characters $c$ that are in common within a window that is of size $\lfloor \frac{\text{max}(\SizeOf{\sigma_1}, \SizeOf{\sigma_2})}{2}\rfloor-1$.
Moreover, the Jaro similarity function also accounts for the number of transpositions $t$, i.e., adjacent characters that are swapped in the two strings.
Put together, the metric is defined as follows:

\begin{equation}
    s_{jaro}(\sigma_1, \sigma_2) =
    \begin{dcases}
        \frac{1}{3} \left( \frac{c}{\vert \sigma_1 \vert} + \frac{c}{\vert \sigma_2 \vert} + \frac{c-t}{c} \right) & \text{if } m = 0 \\
        0 & \text{otherwise}
    \end{dcases}
\end{equation}

In the context of record linkage it makes sense to spend time on improving approximate similarity functions on a specific use case, namely for names.
Names regularly appear abbreviated in text, but this does not change the prefix.
The Jaro-Winkler similarity function takes this into account by increasing the similarity value based on the number of agreeing characters at the beginning of the two strings.
It is computed as follows:

\begin{equation}
    s_winkler(\sigma_1, \sigma_2)=s_jaro(\sigma_1, \sigma_2) + (1-s_jaro(\sigma_1, \sigma_2)\frac{p}{10}
\end{equation}

\noindent with $p\in\{0,\dots,4\}$ being the length of the longest common prefix of at most $4$.

\begin{figure}
    \centering
    \begin{tabular}{?c | ^c | ^c | ^c | ^c | ^c | ^c | ^c |}
        \rowstyle{\itshape}
        Index      & 0 & 1 & 2 & 3 & 4 & 5 & 6 \\\hline
        $\sigma_1$ & \textbf{j} & \textbf{o} & n & e & s &   &   \\		
        Matches    & 0 & 1 & 4 & - & 3 & - & - \\		
        $\sigma_2$ & \textbf{j} & \textbf{o} & h & s & n & o & n \\\hline
    \end{tabular}
    \caption{This table shows the matching of individual characters between string 'jones' and 'johsnon'. The longest sequence of matching starting characters is $2$ and is printed in bold face.}
    \label{fig:jaro_winkler}
\end{figure}

\cref{fig:jaro_winkler} shows how individual characters in the strings 'jones' and 'johsnon' are matched.
We can see that four characters can be matches and there is one transposition, since one character must be swapped to turn the list of indexes into a sorted array.
The Jaro simmilarity can be computed as follows.

\begin{equation*}
    s_{jaro}('jones', 'johsnon') = \frac{1}{3} \left(\frac{4}{5} + \frac{4}{7} + \frac{3}{4} \right) = 0.70714
\end{equation*}

\noindent The number of matching starting character is $2$, which is used by the Jaro-Winker distance to increase the similarity as follows:

\begin{align*}
    s_{winkler}('jones', 'johsnon') &= s_{jaro}('jones', 'johsnon') + (1-s_{jaro}('jones', 'johsnon')\frac{p}{10} \\
    &= 0.70714 + (1 - 0.70714)\times\frac{2}{10} \\
    &= 0.76571
\end{align*}


% % % % % % % % % % % % % % % % % % % % % %


\subsubsection{Soundex and editex phonetic similarity}
\label{sec:soundex}

Another class of approximate string similarities is the \emph{Soundex} phonetic similarity.
It aims to take into account the phonetics, i.e., the auditory properties of the strings, with emphasis on the \emph{homophones}.
These are words that are pronounced in the same way, but are written differently, such as road/rode/rowed or seas/sees/seize.
Ambiguity caused by homophones in conversation are often resolved because of context.
This does not apply to names, however, which may cause a name to be misspelled.
An illustrative example of a special case of homophones can be found in datasets SC8 and C53.\footnote{Add references to http://discovery.nationalarchives.gov.uk/details/r/C3613 and http://discovery.nationalarchives.gov.uk/details/r/C13526}
In C53 there is a mentioned of ``Walter de Gloucestre'' and in SC8 a mention of ``Walter de Gloucester''.
If these are indeed the same people -- this is unfortunately impossible to know for certain -- then this misspelling was probably caused by a cross-language homophone.
Such differences in spelling can be dealt with by devising a phonetic string comparison function, such as soundex.

A soundex encoding of a string consists of the first character of the string, followed by three digits that encode the remaining consonants.
Consonants that are often pronounced similarly are mapped to the same character.
In total, procedure is as follows:

\begin{enumerate}
    \item The starting character of the string is used as the first character of the encoding.
    \item Remove all occurrences of `a', `e', `i', `o', `'u', `y', `h' and `w'.
    \item Substitute consonants after the first character with digits as follows:\\
            \begin{align*}
                b, f, p, v &\rightarrow 1 \\
                c, g, j, k, q, s, x, z &\rightarrow 2 \\
                d, t &\rightarrow 3 \\
                l &\rightarrow 4 \\
                m, n &\rightarrow 5 \\
                r &\rightarrow 6
            \end{align*}
\end{enumerate}

Additionally, the following rules are applied\footnote{reference http://www.archives.gov/research/census/soundex.html}:

\begin{itemize}
    \item \textbf{Names with double letters}: whenever the surname has any double letters, they should be treated as one letter.
    \item \textbf{Names with letters side-by-side that have the same soundex code number}: if the surname has different letters side-by-side that have the same number in the soundex coding guide, they should be treated as one letter.
    \item \textbf{Names with prefixes}: if a surname has a prefix, such as `van', `con', `de', `di', `la', or `le', code both with and without the prefix because the surname might be listed under either code.
    \item \textbf{Consonant separators}: if a vowel separates two consonants that have the same soundex code, the consonant to the right of the vowel is coded.
\end{itemize}

A drawback of the soundex encoding is that the rules are specifically geared towards a certain language, English, in this case.
To capture the phonetics of another language a different mapping must be developed and tested.
Variants for the Indian language are available\footnote{http://airccse.org/journal/ijcsea/papers/4314ijcsea03.pdf}.

Soundex encodings can be used in approximate string matching by checking the encodings of the input strings for equality, resulting in a binary number.
A different approach is to apply the idea of treating phonetically similar values equivalently in way comparable to the edit distance.
The similarity function \emph{editex} modifies the cost function of the edit distance slightly.

\begin{equation}
    c_{editex}(a_i, a_j) =
    \begin{dcases}
        0 & \text{if } a_i = a_j \\
        1 & \text{if } a_i \neq a_j \wedge a_i \equiv_s a_j \\
        2 & \text{otherwise}
    \end{dcases}
\end{equation}

\noindent where $\equiv_s$ is the relation that two characters belong to the same equivalence class in soundex, i.e., they map to the same character, such as `b' and `f'.
This approach has proven to be quite successful for matching names\cite{zobel96}.
Since it is computed analogous to the edit distance it is also computed in time $\mathcal{O}(\SizeOf{\sigma_1} \times \SizeOf{\sigma_2})$.


% % % % % % % % % % % % % % % % % % % % % %


\subsection{Reference pair classification}
\label{sec:classification}

% It is important to note that even though we can similarity metrics to perform `fuzzy' matching, name variants can still cause problems and it is better to learn these first and deal with them.


% % % % % % % % % % % % % % % % % % % % % %


\subsubsection{Comparison of binary feature vectors}
\begin{itemize}
    \item Classification is generally done on feature vectors as resulting from the field-based comparisons.
    \item Various methods have been proposed, such as unsupervised, supervised, active-learning and incremental.
    \item Labeled data is often lacking so unsupervised methods are most widely applicable.
\end{itemize}


% % % % % % % % % % % % % % % % % % % % % %


\subsubsection{Probabilistic record linkage}
\begin{itemize}
    \item In probabilistic record linkage the values in the records are weighted according to an external source of statistics.
    \item Give an example which naturally shows how statistics / informative value aids the linkage.
    \item Define equation that is used in the computation.
    \item Discuss the relation to joint entropy?
\end{itemize}


% % % % % % % % % % % % % % % % % % % % % %


\subsection{Leveraging transitivity (?)}
\begin{itemize}
    \item Transitivity can lead to inconsistencies in the inference of the record linker.
    \item However, evidence can be obtained by applying transitivity, since nodes that share a relatively high number of edges, i.e., are graphically similar, might refer to the same entity.
\end{itemize}


% % % % % % % % % % % % % % % % % % % % % %


\subsection{Data fusion (?)}
\begin{itemize}
    \item Though it is not the primary goal of this thesis, it might be interesting to consider an example of a possible next step in the process.
    \item Discuss how an entity-driven web search might aggregate information about a person.
\end{itemize}