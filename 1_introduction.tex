\begin{savequote}[75mm] 
--Each person in the world creates a Book of Life. This Book starts with birth and ends with death. Its pages are made up of the records of the principal events in life. Record linkage is the name given to the process of assembling the pages of this Book into a volume.
\qauthor{Halbert L. Dunn, 1946} 
\end{savequote}


% % % % % % % % % % % % % % % % % % % % % %


\chapter{Introduction}
\label{ch:introduction}

\chaplettrine{E}{ntity resolution} is the process of finding records in one or more datasets that relate to the same entity.
Entities in this context commonly refer to people, such as patients and customers, but they can also refer to products, events or any other concept.
Applications of entity resolution range from duplicate detection to the merging of datasets to obtain a single, enriched database.
It has a long history of research and a lot of effort has been put into building systems that can perform the task in diverse contexts.
However, many of the techniques developed rely on a database that has a well-defined data model in which specific pieces of information about an entity are mapped to a fixed number of fields.
Whenever unstructured, continuous text is considered, such as commonly found in books and on the web, many of these techniques are not directly applicable and must be modified or perhaps entirely new approaches must be considered.
In this thesis, we explore methods to overcome these issues and study ways of extracting information from text in order to improve entity resolution.


% % % % % % % % % % % % % % % % % % % % % %


\section{Motivation}
\label{sec:motivation}

When studying historical documents, researchers are often confronted with the problem of ambiguous references to people in text.
A document might have been prepared to be of only temporary use or might have been written with a particular set of people in mind, limiting the number of possible people that could have been referred to.
However, when the document is studied after many centuries, the exact context is often lost and it will be more difficult to identify the people.
A careful study of other datasets, such as birth registers and censuses, might therefore be needed to figure out who the person under consideration is.
This can be a laborious task and, furthermore, it can be hard to estimate the quality of the results.

In $2014$, The National Archives (TNA) launched the Traces Through Time project that had the goal of developing a new tool that would enable its users to explore TNA's vast repository of digitized archives in a completely new way.
The system would have to identify occurrences of people in text and decide which of them referred to the same people.
It would also have to deal with ``fuzzy'' nature of historical data, including aliases, incomplete information, spelling variations and errors.
This is essentially an entity resolution problem with the added difference that not structured databases, but unannotated documents in natural language are to be linked.
This projects shows the need for the development of such a system and serves as an excellent use case.

Another possible application of a system for entity resolution in unannotated data lies in the semantic web \citep{Berners2001, Mihalcea2007}.
The semantic web can be seen as a graph in which the nodes represent concepts which are connected by edges that represent a type of relation.
Although the web standards have been extended to support several types of annotations with RDF \citep{Pan2009}, it is not used in all pages on the web, often because it is laborious to incorporate semantic information.
In this context, entity resolution could allow for automatic detection of people in text and link documents together that contain information about the same people.

Information could also be extracted from text and aggregated into automatically generated summaries, resembling simple Wikipedia \citep{Wikipedia} pages.
Web searches could be \emph{entity-driven}: besides the existing ``Images'' and ``News'' categories of search, the search engine could include one for ``People''.
Results from such a system would aggregate web pages referring to the same people together, instead of returning a large list of possibly unrelated people.


% % % % % % % % % % % % % % % % % % % % % %


\section{Challenges}
\label{sec:challenges}

Traditionally, entity resolution is performed on tables in databases.
Since these usually have a schema, and therefore a well-defined structure, we can compute similarities between references by comparing their individual fields.
However, the main focus of this thesis is to study entity resolution in data in natural language, which often lacks a well-defined structure.
Even though the text commonly respects the syntax of a language and therefore has a certain degree of structure, it is hard to parse and ambiguity is hard to avoid.
Furthermore, the references are not readily accessible and first need to be extracted from text.
We will describe a simple generic rule-based method that can be used to both extract references from text and segment it into a record consisting of attributes.
Note that the use of a schema imposes a fixed length on the records, even though it might often happen that one or more attributes are not present.
This becomes more clear with an example.
The following is a section taken from \emph{Fine Roll C 60/33}\citep{FineRolls} from the Fine Rolls of King Henry III, a dataset consisting of transcribed medieval calendars.

\begin{table}
	\centering
	\input{tables/segmentation.tex}
	\caption{A possible segmentation of the paragraph taken from \emph{Fine Roll C 60/33}.}
	\label{tab:segmentation}
\end{table}

\begin{quote}
	Concerning the corn of \ul{Roger of Hyde}. Order to the \ul{sheriff of Oxfordshire} to make the kingâ€™s advantage without delay, by the view of law-worthy men, from all of the corn of \ul{Roger of Hyde, knight}, in Hyde, who is with the Earl Marshal, and to put in gage etc. all those who he will find threshing that corn and intermeddling with the land of the same \ul{Roger} without warrant, to be before the king at his command to answer for it.
\end{quote}

\noindent It is not always clear in which way the various properties that are mentioned should be treated.
For example, is ``sherrif of Oxfordshire'' one role or is ``sherrif'' a role and should ``Oxfordshire'' therefore be treated as a provenance?
The output of the occurrence extraction process therefore depends on various decisions that are made beforehand.
When such decisions are made, the decisions can be captured in a grammar that is used by a parser to segment the input text into records.
This is what we will call a rule-based system.

An example of the tabular output can be seen in \cref{tab:segmentation}.
The nouns that refer to people through their jobs, such as king and Earl Marshal, are left out in this example.
The important thing to note is that there are a lot of missing values that have to be dealt with appropriately.

While text in natural language may lack a well-defined structure, it obviously does not mean it lacks information.
Most of the time the text contains a lot of details and a person is referenced within that context, so it might be interesting to look at ways to extract this information.
A problem with a rule-based system in this case is that it does not generalize very well.
It is fast and can make good use of regularities in the text, such as the ordering of names, but it is unclear how to apply such rules to extract more general information from the text.
We will therefore study the application of more generic techniques and their impact on the accuracy of the system.

The core of any record linkage system is the step in which references are classified into matches and non-matches.
In order to do so, a similarity-based approach is often used.
Based on the assumption that references to the same entity are themselves similar, we can start comparing their segmented records.
However, this requires a means of comparing records, which is not a trivial task.
There are many different approaches, such as counting the number of edit operations required to turn one string into the other, and each comes with their own quirks and advantages.
One of the challenges here is to select a similarity metric and another is to deal with the missing data, such as exemplified in \cref{tab:segmentation}.

Another issue arises when we take the property of \emph{transitivity} into account, i.e., if $a$ and $b$ are a match and $a$ and $c$ are a match, then it is reasonable to assume that also $b$ should be a match with $c$.
However, in some situations, it might occur that this property does not hold, thus we find that there is an inconsistency in the matching status of the pairs under consideration.
Matches can also chain together in which case the ends of the chain are clearly not matches, though intuitively, and by applying the property of transitivity, we could argue that these records should also be matched.
\cref{fig:transitive_closure} shows an example of such an uncertain situation.
We could argue that the same person was referenced once with his first name and another time with middle name, yet we could also say that there is an inconsistency and perhaps only one pair should be matched.
However, we could also argue that the transitivity property can aid us in the efficient linking of records as it can in fact provide evidence in a similar way that the content of the records do.
Namely, instead of comparing references based on their textual representation we could compare them based on graph similarity.
We will look at methods to leverage this property to increase performance while reducing the number of inconsistencies.

\begin{figure}
    \centering
    \input{graphs/transitive_closure.tex}
    \caption[Graphical depiction of transitive closure]{John William Smith has a reasonably high confidence score with both John Smith and William Smith. Inferring from this that also John Smith and William Smith should be matched is however dangerous.}
    \label{fig:transitive_closure}
\end{figure}

The record linkage methods described in this thesis rely on the comparison of references based on their attributes.
Assuming we have no prior knowledge of the match status of each reference pair, and the goal is to link references within a single dataset, the comparison of all possible pairs takes time $\mathcal{O}(n^2)$.
On the other hand, the number of true matches can be expected to increase only linearly with the size of the dataset, thus relatively more time is spent on unpromising pairs.
For larger datasets this problem rapidly becomes too expensive.
Several solutions have been proposed to partially resolve this problem at the expense of decreased sensitivity of the system.
It is not the main focus of this thesis to address this issue, but since it is fairly standard step in the record linkage pipeline, we will shortly discuss it in \cref{sec:blocking}.

The last hurdle is that of evaluation.
Since the data is concerned with people that have all deceased, it is impossible to be certain about the results as produced by a record linkage procedure.
The best we can do is to have an expert historian establish for each pair of references whether they are the same person or not.
Obviously this requires a lot of manual work, and even if such an assessment can be carried out, it will certainly include a fair amount of errors.
Evaluations can therefore prove to be quite a challenge and it is one of the objectives of this thesis to study various approaches of evaluating the results.
The outcomes should be both \emph{scientific}, in the sense that they are comparable, and \emph{insightful}, in that they provide historians with new ways of looking at the data that was not possible before.


% % % % % % % % % % % % % % % % % % % % % %


\section{Problem definition}
\label{sec:problem_definition}
The goal of the Traces Through Time project was to develop a system that could aid historical research of individuals.
This thesis comprises both a report of the development of the system in this project and an overview of record linkage with an emphasis on historical documents.
We will also address the problems that were taken up in \cref{sec:challenges}.
Before we proceed, let us first give a more formal introduction to the concepts discussed in this thesis.

For several decades, record linkage has seen a lot of interest in many different disciplines, which, ironically, led to a large number of synonymic names for the same process, such as data linkage, entity resolution, object identification, or field matching \citep{Christen2012}.
Since two slightly different, but strongly related, problems are addressed, two different naming conventions are taken up in this thesis, in order to distinguish them.
We will denote \emph{entity resolution} and \emph{record linkage} as two distinct tasks, such that we can distinguish between the main objective of aggregating personal information and linking structured records, respectively.
Before defining these concepts more formally, we first define their constituents.

In order to define what we mean with entity resolution, let us first define what we mean with entities and how their descriptions are defined.
Since it is hard to define a domain that will be suited for any entity, we will use references to entities in an unknown domain $E$.
Assume we can obtain information about entities through an `oracle' function $Q$, defined on an entity $\bm{e} \in E$ and a property key $k \in \Sigma^{*}$ that maps to a proper value if the entity possesses the property and a `missing value' $\epsilon$ otherwise.
Using alphabetic characters as keys, we can for example obtain the number of corners of a cube: $Q(\descx{cube}, \stringify{corners}) = 8$.

A \emph{description} $\desc$ consists of a tuple of keys $K$ and values $V$,
e.g. $\descx{cube}=((\stringify{corners}, \stringify{sides}), (8, 6))$.
We denote with the $\cardin{\desc}$ the \emph{description length}, defined as $\cardin{K}=\cardin{V}$, thus $\cardin{\descx{cube}}=2$ in the previous example.
A description $\desc = (K, V)$ is said to describe an entity $\entity$, denoted with $\desc \describes \entity$, whenever the properties of the description match that of the entity, i.e., $Q(k_i, \entity) = v_i$ with $i = 1, 2, \dots, \cardin{\desc}$.
To allow for small mistakes and variation in the data, we will describe more precisely how to compare properties in \cref{sec:field_comparisons}.

We can now describe the process of entity resolution as follows:

\begin{definition}[Entity resolution]
    Entity resolution is the process of deriving from a set of descriptions $\Delta$ another set of descriptions $\hat{\Delta}$ s.t. each of the described entities $\entity \in E$ are described by \emph{at most one description} (injective, non-surjective) and \emph{no information is lost}, i.e.,
    \begin{equation*}
        \hat{\Delta} = \left\{ \bm{x} \describes \entity \mid \bm{x} \describes \entity \wedge \bm{y} \describes \entity \implies \bm{x} = \bm{y}, \forall \bm{x}, \bm{y} \right\}
    \end{equation*}
\end{definition}

Note that even in the presence of an oracle $Q$, we could never know if a description truly references an entity, since the non-surjective property implies that a description can describe several elements in $E$.
What makes matters even more complicated is that we also lack knowledge of the set $E$.
The best we can do is assume that descriptions describe at least one entity in $E$ and `ground' the oracle to it, i.e., we treat descriptions as incomplete entities.
This allows us to do pairwise comparisons of descriptions as the core component of the entity resolution process.
Whenever a query involves an unknown property, an $\epsilon$ is returned.

Also note that from the fact that there is an injective, non-surjective \emph{describes}-relation between $\hat{\Delta}$ and $E$ and that no information is lost, it is implied that descriptions in the original set $\Delta$ are \emph{merged} during the record linkage process.
Entity resolution therefore focuses on the construction of a set of unique descriptions, which involves the aggregation of information whenever two partial descriptions exist for the same (real-world) entity.
Record linkage, however, is only concerned with the detection of the latter and not with the aggregation itself.

\begin{definition}[Record linkage]
    Given a set of (partial) descriptions $\Delta$, record linkage is the process of determining the set of \emph{matching pairs} $M$, or simply \emph{matches}, defined as:
    \begin{equation*}
        M = \left\{ (\bm{x}, \bm{y}) \mid \bm{x} \describes \entity \wedge \bm{y} \describes \entity, \exists \entity \in E, \forall \bm{x}, \bm{y} \in \Delta \right\}
    \end{equation*}
    \label{def:record_linkage}
\end{definition}

Again, by grounding the oracle to a specific description, we can do pairwise comparisons between descriptions.
Note that the relation defined in \cref{def:record_linkage} is \emph{symmetric}, which limits the total number of possible matches to $n(n-1)/2$, if we do not compare descriptions with themselves.

Note that we assume that, in practice, it is impossible to verify whether both $a$ and $b$ are partial descriptions of $c$ since this would require knowledge of their match status -- a ``chicken-and-egg'' problem.
This shows an important problem in record linkage, namely that there is often a lack of a ground truth.
Even if expert knowledge is available, it can be argued that it is impossible to verify any links that are made, since it requires complete knowledge of the set of unique objects, which would invalidate the need to perform entity resolution in the first place.

\begin{figure}
    \centering
    \bgroup
    \def\arraystretch{1.1}
    \setlength{\tabcolsep}{12pt}
    \begin{tabular}{c c | c | c |}
        & \multicolumn{1}{c}{} & \multicolumn{2}{c}{{\small \textit{Predicted label}}} \\ %\cline{2-3}
        & & \textbf{Positive} & \textbf{Negative}  \\ \cline{2-4}
        \multirow{2}{*}{\rot{{\small \textit{True label}}}} & \textbf{Positive} & \begin{tabular}[x]{@{}c@{}}true\\positive\end{tabular} & \begin{tabular}[x]{@{}c@{}}false\\negative\end{tabular} \\ \cline{2-4}
        & \textbf{Negative} & \begin{tabular}[x]{@{}c@{}}false\\positive\end{tabular} & \begin{tabular}[x]{@{}c@{}}true\\negative\end{tabular} \\ \cline{2-4}
    \end{tabular}
    \egroup
    \caption{A confusion matrix shows the performance of a system using the frequencies of the four possible outcomes.}
    \label{fig:confusion_matrix}
\end{figure}

The record linkage problem can be viewed as a \emph{binary classification} problem in which to goal is to distinguish between \emph{matches} and \emph{non-matches}, respectively labeled as $1$ and $0$.
When the \emph{ground truth} about the data is known, i.e., the actual labels are available, the outcome of classification can be evaluated.
\cref{fig:confusion_matrix} shows the organization of a \emph{confusion matrix} that can be used for evaluation.
The cells contain the frequencies for the four possible outcomes of a classified pair: \emph{true positive} (TP), \emph{false negative} (FN), \emph{false positive} (FP) and \emph{true negative} (TN).
These frequencies can be aggregated using metrics that describe different aspects of the results.

\begin{definition}[Precision]
    Precision, $E_{p}$, is the fraction of pairs that are correctly classified as true matches, i.e.,
    \begin{equation*}
        E_{p} = \frac{ \abs{\text{TP}} }{ \abs{\text{TP}} + \abs{\text{FP}} }
    \end{equation*}
\end{definition}

\begin{definition}[Recall]
    Recall, $E_{r}$, is the fraction of true matches that are detected by the system, i.e.,
    \begin{equation*}
        E_{r} = \frac{ \abs{\text{TP}} }{ \abs{\text{TP}} + \abs{\text{FN}} }
    \end{equation*}
\end{definition}

In general, it holds that a system that yields results of high precision often scores low on recall, and vice versa.
Indeed, a maximum precision of $1$ can be achieved when no pairs are classified as matches, though this would yield a very low recall.
Similarly, classifying all pairs as matches would yield the maximum recall of $1$, but the precision would be very low.
This shows that choices have to be made according to the cost associated with lower precision or recall, based on the context in which record linkage is applied, and it is beneficial to study both aspects.
There exist metrics that also include the number of true negatives, but in record linkage these are problematic, since this number is often very high, making it dominate the equation \citep{Christen2012}.

It is also possible to aggregate precision and recall into a single metric, such as the \emph{F-measure}.

\begin{definition}[F-measure]
    The general F-measure measures the effectiveness of retrieval with respect to a user who attaches $\beta$ times as much importance to recall as precision \citep{Rijsbergen1979} in the following way:
    \begin{equation*}
        F_{\beta} = (1 + \beta^{2}) \cdot \frac{2 E_{p} E_{r}}{(\beta \cdot E_{p}) + E_{r}}
    \end{equation*}
    \label{def:fmeasure}
\end{definition}

\noindent where $\beta \geq 0$.

Commonly used values for $\beta$ are $1$, $2$ and $0.5$. The F\textsubscript{1} weighs precision and recall evenly, while the F\textsubscript{2} and F\textsubscript{0.5} put more emphasis on recall and precision, respectively.
The class of F-measures is interesting because it gives a higher penalty to extreme values when compared to the arithmetic mean, which is in many cases desirable.
As mentioned, scoring very high on either precision or recall is easy to achieve and should generally be avoided.

Now that we have a formal definition of record linkage we will pose the the research question that will be leading throughout this thesis:

\begin{question*}
    How can we perform entity resolution to aid the historical research in the absence of tabular structured archive material?
\end{question*}

\noindent This question is divided into several sub-questions:

\begin{enumerate}[label=\emph{\roman*.}]
    \item How do we transform the source data into a format that can be used by traditional record linkage techniques?
    %\item Do we need to transform the source data into a tabular format or can we perform entity resolution without such structure?
    \item What methods can be used to extract information from documents to improve the performance of entity resolution?
\end{enumerate}


% % % % % % % % % % % % % % % % % % % % % %


\section{Related work}
\label{sec:related_work}

The Traces Through Time project started as a continuation of the ChartEx \citep{Knobbe2014} project that had the goal of developing new ways of analyzing historical documents in an integrated fashion and reconstructing medieval social networks.
In ChartEx the focus was on medieval charters, which are records of legal transactions of property, while Traces Through Time considered only people.
This thesis gives an overview of the work that was done in the latter, meaning that the focus will also be on people.

The goal of these projects is not much different from the one proposed in \citep{Dunn1946}, namely to aggregate all significant information about a person from birth to death into a single record.
In \citep{Newcombe1959} the \emph{probabilistic record linkage} method was proposed as a solution to this problem, which was later formalized in \citep{Fellegi1969}.
This method is based on attaining evidence that supports the hypothesis that two distinct records are descriptions of the same real-world entity.
In \cref{sec:classification} we will describe a procedure that is based on the same principle and differences in the way statistics about the various fields are incorporated.

The research done in \citep{Cucerzan2007} and \citep{Bunescu2006} is similar to that conducted in this thesis, although the domain is different.
Their goal is also to perform record linkage in text and both use Wikipedia as a source of evidence.
Linking ``out-of-Wikipedia''\citep{Bunescu2006} entities is achieved by introducing a special entity $e_{out}$ that has all its attributes set to null values.
Experimental evaluation of these systems has shown that using external knowledge from Wikipedia, i.e., information not present in the data that contains the references that are to be matched, enhances the accuracy.

The work described in \citep{Winchester1970} is interesting since it is one of the earliest mentions of record linkage in a historical setting and executed in close collaboration with historians.
The involvement of historians resulted in some requirements that computer scientists might miss, such as allowing researchers to document the information that guided the decision making process.
This is something that was also requested by the historians that we cooperated with during the Traces Through Time project.
The paper also reports some of the major difficulties found during manual linkage of two censuses that took five months, most notably the high discrepancy between the way names and occupations were written.

In the recent work of \citep{Balado2015}, the entity resolution problem is studied in a historical context.
The goal of this work is to link mentions of ship names in news paper articles with a curated list of ship names and crew members.
The research is similar to that conducted as part of this thesis in that entities in natural language are disambiguated.
The most important difference is that the researchers obtained a labeled dataset through careful inspection of the text by domain experts, which was used to train a classifier.
The work proposed in this thesis aims to provide methods for unsupervised learning instead.


% % % % % % % % % % % % % % % % % % % % % %


\section{Outline}
\label{sec:outline}

The next chapters describe the research that has been conducted to answer the research question posed in the previous section.
The theory of record linkage is first discussed in \cref{ch:record_linker}, which will lay the foundation for further research.
In it, the various steps involved in their order of appearance as is common in a record linkage pipeline, is studied.
\cref{ch:feature_extraction} explores methods for extracting information from unannotated text in an unsupervised way with the aim of improving the results in the presence of common names.
The results of experimental evaluation that was conducted will then be described in \cref{ch:experiments}, followed by our conclusions in \cref{ch:conclusions}.